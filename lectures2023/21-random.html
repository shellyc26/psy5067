<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Random Topics</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.19/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Random Topics
]

---





## Last time...

Logistic Regression 

## Today

* Review &amp; finish logistic regression
* Weighted Least Squares
* Other random `R` things

---

## Estimation with Maximum Likelihood

- Logistic regression is used when our outcomes are categorical
- OLS minimizes the errors ( `\(SS_{res}\)`), which maximizes ( `\(SS_{reg}\)`)   
- In logistic regression we are not so lucky  

- Need to rely on iterative procedure, ML Estimation:

  - Pick parameters of your model ( `\(b_0\)`, `\(b_1\)` etc. ), and calculate the *likelihood* of the data, given those parameters. We do this iteratively until we find the best parameters -- the ones that *maximize* the *likelihood* of your data. 

---
## GLM in R

.pull-left[

```r
glm(formula,
*   family = gaussian(link="identity"),
    data,
    weights,
    subset,
    na.action,
    start = NULL,
    etastart,
    mustart,
    offset, 
    control = glm.control(...),
    model = TRUE,
    method = ”glm.fit”,
    x = FALSE, 
    y = TRUE,
    contrasts = NULL, ...)
```
]

.pull-right[
The `family` argument specifies the distribution. In R, families have default links. 

![](images/glmfamily.png)
]

---
## GLM in R

.pull-left[

```r
*glm(y ~ X1+ X2 + X3 ,
    family = binomial,
    data = dataset)
```
]

.pull-right[
Specify the model like you would with `lm`
]

---
## GLM in R

.pull-left[

```r
glm(y ~ X1+ X2 + X3 , 
*   family = binomial,
    data = dataset)
```
]

.pull-right[

Specify the distribution you're working with. When binary outcomes, we'll use the binomial. 

]

---
## GLM in R

.pull-left[

```r
glm(y ~ X1+ X2 + X3 , 
    family = binomial, 
*   data = dataset)
```
]

.pull-right[

Specify your dataset.

]

---
## How to interpret
- `\(b_1\)` is the predicted change in the logit for a 1-unit change in X, holding the other predictors constant 

- For a 1-unit change in X, holding other predictors constant, the odds that Y = 1 changes by `\(e^{b_{1}}\)`   

  - e.g,. `\(b_{1}\)` = .4, `\(e^{.4}\)` = 1.49 

- For fitted values, need to use entire equation
`\(\hat{Y} = e^{b_{0}+b_{1}X_{1}}\)`

- Turn to probabilities by: `\(\frac{\text{odds}}{(1 + \text{odds})}\)`

---
## Example




```r
# 1 = not premature
mortality
```

```
## # A tibble: 300 × 4
##    Intelligence_Self Intelligence_Mate premature.d NOT.premature
##                &lt;dbl&gt;             &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;
##  1                22                19 normal                  1
##  2                22                18 normal                  1
##  3                21                21 normal                  1
##  4                22                17 normal                  1
##  5                19                18 normal                  1
##  6                19                20 premature               0
##  7                16                18 normal                  1
##  8                15                11 premature               0
##  9                16                21 normal                  1
## 10                19                22 normal                  1
## # … with 290 more rows
```

---


```r
death.1 &lt;- lm(NOT.premature ~ Intelligence_Self , data = mortality)
summary(death.1)
```

```
## 
## Call:
## lm(formula = NOT.premature ~ Intelligence_Self, data = mortality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.9030  0.1084  0.1538  0.1907  0.3355 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       0.641769   0.098636   6.506 3.25e-10 ***
## Intelligence_Self 0.011357   0.005807   1.956   0.0514 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.3745 on 298 degrees of freedom
## Multiple R-squared:  0.01267,	Adjusted R-squared:  0.009362 
## F-statistic: 3.826 on 1 and 298 DF,  p-value: 0.05141
```


---


```r
death.2 &lt;- glm(NOT.premature ~ Intelligence_Self , data = mortality)
summary(death.2)
```

```
## 
## Call:
## glm(formula = NOT.premature ~ Intelligence_Self, data = mortality)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9030   0.1084   0.1538   0.1907   0.3355  
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       0.641769   0.098636   6.506 3.25e-10 ***
## Intelligence_Self 0.011357   0.005807   1.956   0.0514 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for gaussian family taken to be 0.1402466)
## 
##     Null deviance: 42.330  on 299  degrees of freedom
## Residual deviance: 41.793  on 298  degrees of freedom
## AIC: 266.05
## 
## Number of Fisher Scoring iterations: 2
```


---

```r
anova(death.1)
```

```
## Analysis of Variance Table
## 
## Response: NOT.premature
##                    Df Sum Sq Mean Sq F value  Pr(&gt;F)  
## Intelligence_Self   1  0.537 0.53653  3.8256 0.05141 .
## Residuals         298 41.793 0.14025                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---

```r
anova(death.2)
```

```
## Analysis of Deviance Table
## 
## Model: gaussian, link: identity
## 
## Response: NOT.premature
## 
## Terms added sequentially (first to last)
## 
## 
##                   Df Deviance Resid. Df Resid. Dev
## NULL                                299     42.330
## Intelligence_Self  1  0.53653       298     41.793
```

---


```r
death.3 &lt;- glm(NOT.premature ~ Intelligence_Self,
               family = binomial, data = mortality)
summary(death.3)
```

```
## 
## Call:
## glm(formula = NOT.premature ~ Intelligence_Self, family = binomial, 
##     data = mortality)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1175   0.4923   0.5716   0.6438   0.9943  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)        0.28695    0.67490   0.425   0.6707  
## Intelligence_Self  0.08012    0.04143   1.934   0.0532 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 273.53  on 299  degrees of freedom
## Residual deviance: 269.75  on 298  degrees of freedom
## AIC: 273.75
## 
## Number of Fisher Scoring iterations: 4
```

---
## Modified on 4/20

For a 1-unit change in X, holding other predictors constant, the odds that Y = 1 changes by `\(e^{b_{1}}\)`


```r
exp(1)^.08012
```

```
## [1] 1.083417
```

*For every 1-unit increase in Intelligence, the odds of living increase by 8%*

---
## Interpreting Odds Ratios

![](images/oddratios.png)
.small[
[Idiot's Guide to Odds Ratios](https://journalfeed.org/article-a-day/2018/idiots-guide-to-odds-ratios/)
]

---
## Specific Values?

What if you want the probability of being a premature death for a given level of Intelligence? (Now that we've run our model and have parameters...)

For fitted values, need to use entire equation
`\(\hat{Y} = e^{b_{0}+b_{1}X_{1}}\)`


```r
# get fitted value with a given value of X (here 20)
exp(1)^(0.28695 + (.08012*20))
```

```
## [1] 6.615067
```

```r
# now get odds
6.615067 / (1+6.615067)
```

```
## [1] 0.8686814
```

---
## Probit 

We can have different link functions. When your response variable (DV) is truly binary -- the data generating process generates legit binary data -- logit is your pick.

--

What if your response variable is binary, but the underlying construct you are trying to measure is likely Gaussian? Ex: depressed vs. not depressed. But the underlying latent construct is continuous. More appropriate then is the **probit** link function.

[Stack exchange thread if you're going down this route](https://stats.stackexchange.com/questions/20523/difference-between-logit-and-probit-models)
---
## Probit



```r
death.4 &lt;- glm(NOT.premature ~ Intelligence_Self,
        family = binomial(link = "probit"), data = mortality)
summary(death.4)
```

```
## 
## Call:
## glm(formula = NOT.premature ~ Intelligence_Self, family = binomial(link = "probit"), 
##     data = mortality)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1265   0.4889   0.5723   0.6454   0.9750  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)        0.21961    0.38376   0.572   0.5671  
## Intelligence_Self  0.04513    0.02319   1.946   0.0516 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 273.53  on 299  degrees of freedom
## Residual deviance: 269.72  on 298  degrees of freedom
## AIC: 273.72
## 
## Number of Fisher Scoring iterations: 4
```

---
## Expanding upon simple logistic models

- You can include covariates
- You can have interactions...BUT

**Interactions are super hard to interpret in logistic regression**

--

- We have nonlinear mapping (S-function)
- Can express in terms of odds, probabilities, or logits
- Whether your observe an interaction *depends* on if you express the outcome in terms of odds, probabilities, or logits...You can get very different results!
- Ultimately, you might want to use other techniques

---
class: inverse, center, middle

# Weighted Least Squares

---
## Estimation methods thus far

- OLS
- MLE 

--

Why do we need another method?

---

## Homoscedasticity

Homoscedasticity is the assumption that the variance of Y is constant across all levels of a predictor.



![](21-random_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;

Do we meet this assumption?
---

## Weighted least squares

Weighted least squares (WLS) is a commonly used remedial procedure for heteroscedasticity.

In an ordinary least squares (OLS) approach, each case in the dataset is given equal weight.
  - WLS assigns each case a weight `\(w_i\)`, depending upon the precision of the observation of Y in that case.
  
  - For observations for which the variance around the residuals around the regression line is low, the case is given a high weight.

---

Recall that an OLS estimation chooses values of `\(b_0\)` and `\(b_1\)` that minimizes the sum of squared residuals:

`$$\large \text{min}(\sum e^2_i) = \text{min} \sum (Y_i-b_1X_i-b_0)^2$$`

In a WLS approach, weights are taken into account, such that the values of `\(b_0\)` and `\(b_1\)` are chosen to minimize the sum of the **weighted** squared residuals:

`$$\large \text{min}(\sum w_ie^2_i) = \text{min} \sum w_i(Y_i-b_1X_i-b_0)^2$$`

The value of the weights is the inverse of the conditional variance of the residuals in the population corresponding to the specified value of X:

`$$\large w_i = \frac{1}{\sigma^2_{Y-\hat{Y}|X}}$$`
---

The value of `\(\sigma^2_{Y-\hat{Y}|X}\)`, the variance of the residuals in the population conditional on X, is not known and must be estimated. A common procedure for estimating weights is to estimate the usual OLS regression equation, square the residuals, and then regress the squared residuals onto X. The weight is then estimated as the inverse of the predicted value for a case. 

Using our own data:


```r
ols.model = lm(problems ~ calm, data = Data)
library(broom)
ols_aug = augment(ols.model)
head(ols_aug)
```

```
## # A tibble: 6 × 8
##   problems  calm .fitted .resid   .hat .sigma .cooksd .std.resid
##      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;
## 1        4 1.06     4.87 -0.872 0.0326  0.665 0.0296      -1.33 
## 2        5 1.24     4.70  0.304 0.0278  0.671 0.00306      0.462
## 3        6 1.30     4.63  1.37  0.0263  0.653 0.0581       2.07 
## 4        4 0.852    5.08 -1.08  0.0391  0.660 0.0558      -1.66 
## 5        5 1.12     4.82  0.180 0.0311  0.672 0.00120      0.273
## 6        5 0.715    5.22 -0.223 0.0438  0.672 0.00266     -0.341
```
---


```r
# square residuals
ols_aug$resid_sq = ols_aug$.resid^2
# regress squared resid on predictor
weight.mod = lm(resid_sq ~ calm, data = ols_aug)
```

.pull-left[

```r
coef(ols.model)
```

```
## (Intercept)        calm 
##    5.941940   -1.005699
```

]

.pull-right[

```r
coef(weight.mod)
```

```
## (Intercept)        calm 
##   1.0739505  -0.2591579
```

]


```r
# extract predicted values
pred.resid = predict(weight.mod)
head(pred.resid)
```

```
##         1         2         3         4         5         6 
## 0.7982546 0.7527584 0.7366885 0.8530291 0.7849316 0.8886457
```

```r
# find inverse of predicted values
# use absolute value if some of your predicted values are negative  
est.weights = 1/abs(pred.resid) 
head(est.weights)
```

```
##        1        2        3        4        5        6 
## 1.252733 1.328447 1.357426 1.172293 1.273996 1.125308
```


---


```r
wls.model = lm(problems ~ calm, data = Data, weights = est.weights)
```


#### OLS solution

```r
tidy(ols.model)
```

```
## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)     5.94    0.182       32.6 3.54e-47
## 2 calm           -1.01    0.0675     -14.9 1.58e-24
```

#### WLS solution

```r
tidy(wls.model)
```

```
## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)    5.64     0.182       30.9 1.60e-45
## 2 calm          -0.884    0.0451     -19.6 7.58e-32
```

---

![](21-random_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;


---
## When to use WLS

WLS is a **robust** method of estimation. "**Robust statistics** are statistics with good performance for data drawn from a wide range of probability distributions, especially for distributions that are not normal." .small[_(this quote taken straight from the [Wikipedia page](https://en.wikipedia.org/wiki/Robust_statistics))_]

Use when...
- Dealing with heteroscedasticity

- You know already that points should not be treated equally (some should be weighed more than others); do you know that some of your data was measured with less error than other data?

---
class: inverse, center, middle

# Other Random R Things

---

## Checking Assumptions


```r
library(performance)
library(palmerpenguins)

model1 = lm(body_mass_g ~ bill_length_mm, data = penguins)
summary(model1)
```

```
## 
## Call:
## lm(formula = body_mass_g ~ bill_length_mm, data = penguins)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1762.08  -446.98    32.59   462.31  1636.86 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     362.307    283.345   1.279    0.202    
## bill_length_mm   87.415      6.402  13.654   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 645.4 on 340 degrees of freedom
##   (2 observations deleted due to missingness)
## Multiple R-squared:  0.3542,	Adjusted R-squared:  0.3523 
## F-statistic: 186.4 on 1 and 340 DF,  p-value: &lt; 2.2e-16
```


---
## Checking Assumptions

.small[

```r
check_model(x = model1)
```

![](21-random_files/figure-html/unnamed-chunk-28-1.png)&lt;!-- --&gt;
]


---
## Bayes Tutorial 

(if time)

---

class: inverse

## End of semester

- Tuesday = Messi talking about resampling methods, namely bootstrapping

- Thursday = Shelly talking about machine learning; READ THE YARKONI &amp; WESTFALL PAPER

- Tuesday = Review session

- Thursday - Monday - Exam 3 will be open
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
