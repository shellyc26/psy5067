---
title: 'Univariate regression II'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["xaringan-themer.css", "my-theme.css"]
    incremental: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
<style type="text/css">
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 50% !important;
}

</style>

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(base_color = "#17139C",link_color = "#DD3E3E")

```


```{r, echo = F, message = F, warning=F}
library(tidyverse)
library(broom)
```


## Last time...

- Introduction to univariate regression

- Calculation and interpretation of $b_0$ and $b_1$

- Relationship between $X$, $Y$, $\hat{Y}$, and $e$ 

---

## Today...

Statistical inferences with regression

- Partitioning the variance

- Testing $b_{xy}$


---

## Statistical Inference

- The way the world is = our model + error

- How good is our model? Does it "fit" the data well? 

--

To assess how well our model fits the data, we simply take all the variability in our outcome and partition it into different categories. For now, we will partition it into two categories: the variability that is predicted by (explained by) our model, and variability that is not.

---

## Partitioning variation 

- We formally test how well we are doing with our guesses by partitioning variation
- To the extent that we can generate different predicted values of Y, based on the values of the predictors, we are doing well in our prediction

$$\sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2$$
---
## Partitioning variation 
$$\sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2$$

Each of these is the sum of a squared deviation from an expected value of Y. We can abbreviate the sum of squared deviations:

$$SS_{Y} = SS_{\text{Model}} + SS_{\text{Residual}}$$


$$\frac{s^2_{regression}}{s^2_y} = \frac{SS_{regression}}{SS_Y} = R^2$$

---
## Partitioning Variance

The relative magnitude of sums of squares, especially in more complex designs, provides a way of identifying particularly large and important sources of variability. In the future, we can further partition $SS_{\text{Model}}$ and $SS_{\text{Residual}}$ into smaller pieces, which will help us make more specific inferences and increase statistical power, respectively. 

$$\Large s^2_Y = s^2_{\hat{Y}} + s^2_{e}$$
---

## Partitioning variance in Y
Consider the case with no correlation between X and Y

$$\Large \hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X})$$
$$\Large \hat{Y} = \bar{Y}$$

To the extent that we can generate different predicted values of Y based on the values of the predictors, we are doing well in our prediction

$$\large \sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2$$

$$\Large SS_Y = SS_{\text{Model}} + SS_{\text{Residual}}$$

---
## Coefficient of Determination

$$\Large \frac{s_{Model}^2}{s_{y}^2} = \frac{SS_{Model}}{SS_{Y}} = R^2$$

$R^2$ represents the proportion of variance in Y that is explained by the model. 

--

$\sqrt{R^2} = R$ is the correlation between the predicted values of Y from the model and the actual values of Y

$$\large \sqrt{R^2} = r_{Y\hat{Y}}$$
---

```{r, echo = F}
set.seed(123)
x.1 <- rnorm(10, 0, 1)
e.1 <- rnorm(10, 0, 2)
y.1 <- .5 + .55 * x.1 + e.1
d.1 <- data.frame(x.1,y.1)
m.1 <- lm(y.1 ~ x.1, data = d.1)
d1.f<- augment(m.1)
```

.pull-left[
```{r, echo = F, warning = F, message=FALSE}
d1.f %>%
  ggplot(aes(x = x.1, y = y.1)) +
  geom_smooth(se = F, method = "lm")+
  geom_point(shape = 1, size = 4) +
  geom_point(aes(y = .fitted), color = "red", size = 4)+
  theme_bw(base_size = 35) 
```
]

--

.pull-right[
```{r, echo = F, warning=FALSE, message=FALSE}
d1.f %>%
  ggplot(aes(x = y.1, y = .fitted)) +
  geom_smooth(se = F, method = "lm")+
  geom_point(color = "black", size = 4) +
  theme_bw(base_size = 35)
```
]




---

.small[
```{r highlight.output = 18}
galton.data <- psychTools::galton
fit.1 = lm(child ~ parent, data = galton.data)
summary(fit.1) 
```
]

---
## Example

```{r}
cor(galton.data$parent, galton.data$child, use = "pairwise")
```

--

```{r}
cor(galton.data$parent, galton.data$child)^2
```

--

```{r}
galton.fits = augment(fit.1)
cor(galton.fits$child, galton.fits$.fitted)
```

--

```{r}
summary(fit.1)$r.squared
```

---
## Computing Sum of Squares

$$\Large \frac{SS_{Model}}{SS_{Y}} = R^2$$
$$\Large SS_{Model} = R^2({SS_{Y})}$$



$$\Large SS_{Y} = SS_{Model} + SS_{residual}$$


$$\Large SS_{residual} = SS_{Y} - R^2({SS_{Y})}$$

$$\Large  SS_{residual} = (1- R^2){SS_{Y}}$$

---

### Using `R` To Check Yourself
$$SS_{residual} = (1- R^2){SS_{Y}}$$
```{r}
r2 = summary(fit.1)$r.squared
fit.1.anova = summary(aov(fit.1))
ssTotal = fit.1.anova[[1]]$`Sum Sq`[1] + fit.1.anova[[1]]$`Sum Sq`[2]
ssResidual = (1 - r2) * ssTotal
fit.1.anova
ssResidual

```



---

## What is an "error"?
 
```{r echo=FALSE, message=FALSE, warning=FALSE}
set.seed(123)
library(broom)
library(tidyverse)
x.1 <- rnorm(10, 0, 1)
e.1 <- rnorm(10, 0, 2)
y.1 <- .5 + .55 * x.1 + e.1
d.1 <- data.frame(x.1,y.1)
m.1 <- lm(y.1 ~ x.1, data = d.1)
d1.f<- augment(m.1)
d1.f$color = "black"
d1.f[9, "color"] = "red"
newcolors = c("black", "red")
```


```{r, echo=FALSE, fig.align='center'}
ggplot(d1.f , aes(x=x.1, y=y.1)) +
  geom_point(size = 2, aes(color = color)) +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE) +
  geom_point(aes(y = .fitted, color = color), shape = 1, size = 2) +
  geom_segment(aes( xend = x.1, yend = .fitted, color = color))+
  geom_hline(aes(yintercept = mean(y.1)))+
  scale_color_manual(values = newcolors)+
  guides(color = "none")+
  theme_bw(base_size = 20)
```

---

### Mean Square Error (MSE) (msw/msr)

- AKA square of residual standard error/deviation ( $\sigma^2$ )
- *Unbiased* estimate of error variance
- Measure of discrepancy between the data and the model
- Variance of data around the fitted regression line (same as MSwithin for group means)
- It is the mean of the square of the residuals

```{r}
head(fit.1$residuals)
mse = round(mean(fit.1$residuals ^ 2), digits = 2)
mse
```

---
## MSE

- It is the square of the residual standard error/deviation (sigma) aka $RSE^2$

```{r}
mse = round((summary(fit.1)[["sigma"]]) ^2, digits = 2)
mse
```

---

## Residual Standard Error

```{r, echo=FALSE, highlight.output = 17}
summary(fit.1)
```

---
## Residual standard error/deviation 

- aka standard deviation of the residual
- aka **standard error of the estimate**
- aka $\sigma$

$$\hat{\sigma} = \sqrt{\frac{SS_{\text{Residual}}}{df_{\text{Residual}}}} = s_{Y|X} = \sqrt{\frac{\Sigma(Y_i -\hat{Y_i})^2}{N-2}}$$

- interpreted in original units (unlike $R^2$)

- standard deviation of Y not accounted by model

---


### RSE


```{r}
summary(fit.1)$sigma 
galton.data.1 = broom::augment(fit.1)
psych::describe(galton.data.1$.resid)
sd(galton.data$child)
```

$\hat{\sigma}$ is in original units of Y, so you need to compare to the sd of Y!

---

## RSE vs MSE

Residual standard error = square root of the mean square error

Both measuring error, but RSE is a little more useful

```{r}
sqrt(mse)
```


---
## Residual Standard Error and $\sigma$

- So many names to represent the spread of data around the regression line
- Standard deviation of the residual, standard error of the estimate, MSE...
- We will refer to this as sigma, and use estimated sigma, as we do not know the population value ( $\hat{\sigma}$ )
- It is interpreted in original units (unlike $R^2$ )
- It is the standard deviation of Y not accounted by the model (i.e., residuals)

---
## Why do we care about $\sigma$?

- Let's simulate!
- Data generating process: $$Y_{i} \sim \mathcal{N}(\mu,\,\sigma)\ $$
- This describes how we think our DVs are generated, and the parameters of interest
- For normal, $\mu$ gets all the focus but $\sigma$ is just as important

---

## Simulation

Our plan is to fix $\mu$ and then vary $\sigma$ to see what happens

```{r}
set.seed(1234)
x.1 <- rnorm(1000, 0, 1) # randomly select 1000 numbers for x
e.1 <- rnorm(1000, 0, 1) # randomly select 1000 numbers for error
y.1 <- .5 + .55 * x.1 + e.1 # create our y
d.1 <- data.frame(x.1,y.1) # combine x and y into a data.frame
m.1 <- lm(y.1 ~ x.1, data = d.1) # use x to predict y with this data
```

---

## Simulation

```{r}
summary(m.1)
```

---

## Simulation

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
library(ggplot2)
ggplot(d.1, aes(x = x.1,y =  y.1)) +
    geom_point() +    
    geom_smooth(method = lm) +
   scale_x_continuous(limits = c(-3, 3))  +
  scale_y_continuous(limits = c(-6, 6)) +
  theme_classic(base_size = 18)
```

---

## Simulation
Again, but with a larger sigma
```{r}
set.seed(987)
e.2 <- rnorm(1000, 0, 2) # larger sigma
y.2 <- .5 + .55 * x.1 + e.2 # same Xs, same mu (.5)
d.2 <- data.frame(x.1,y.2)
m.2 <- lm(y.2 ~ x.1, data = d.2)
```


---

## Simulation
```{r}
summary(m.2)
```

---

## Simulation

```{r,echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
ggplot(d.2, aes(x=x.1, y=y.2)) +
    geom_point() +    
    geom_smooth(method=lm) +
  scale_x_continuous(limits = c(-3, 3))  +
  scale_y_continuous(limits = c(-6, 6)) +
  theme_classic(base_size = 18)
```

---
## $R^2$ and residual standard deviation

- two sides of same coin

- one in original units, the other standardized 

- $R^2$ can be tricky because the numerator and denominator can be changed in different ways. 

- for example if variance in Y is changed but with the same regression model and residual standard error, $R^2$ could increase or decrease

---
```{r, echo = F, fig.align='center'}
set.seed(20200116)
x.1 = rnorm(1000,0,1)
e = seq(4, .01, by = -.1)
r2 = numeric(length = length(e))
rsd= numeric(length = length(e))
for(i in 1:length(e)){
  e.1 <- rnorm(1000, 0, e[i])
  y.1 <- .5 + .55 * x.1 + e.1
  d.1 <- data.frame(x.1,y.1)
  m.1 <- lm(y.1 ~ x.1, data = d.1)
  r2[i] = summary(m.1)$r.squared
  rsd[i] = summary(m.1)$sigma
}

data.frame(r2 = r2, rsd = rsd) %>%
  ggplot(aes(x = r2, y = rsd)) + 
  geom_line() +
  scale_x_continuous(expression(paste(R^2))) + 
  scale_y_continuous(expression(hat(sigma[e])))  +
  theme_bw(base_size = 20)
```
---
## Inferential tests

NHST is about making decisions:
  
  - these two means are/are not different
  - this correlation is/is not significantly different from 0
  - the distribution of this categorical variable is/is not different between these groups
  
--

In regression, there are several inferential tests being conducted at once. The first is called the **omnibus test** -- this is a test of whether the model fits the data. 

---

## Omnibus test

$$\Large H_{0}: \rho_{XY}^2= 0$$

$$\Large H_{0}: \rho_{XY}^2 \neq 0$$

It is possible to calculate the significance of your regression with a correlation test. 

--

However, we use **the _F_ distribution** to estimate the significance of our model. Methods are mathematically equivalent! But when have we seen the **F** before?


---

## _F _ Distribution review

The F probability distribution represents all possible ratios of two variances:

$$F \approx \frac{s^2_{1}}{s^2_{2}}$$
Each variance estimate in the ratio is $\chi^2$ distributed, if the data are normally distributed. The ratio of two $\chi^2$ distributed variables is $F$ distributed. It should be noted that each $\chi^2$ distribution has its own degrees of freedom.

$$F_{\nu_1\nu_2} = \frac{\frac{\chi^2_{\nu_1}}{\nu_1}}{\frac{\chi^2_{\nu_2}}{\nu_2}}$$
As a result, _F_ has two degrees of freedom, $\nu_1$ and $\nu_2$

???

---

## _F_  Distributions and regression

Recall that when using a _z_ or _t_ distribution, we were interested in whether one mean was equal to another mean. In this comparison, we compared the _difference of two means_ to 0 (or whatever), and if the difference was not 0, we concluded significance. 

_F_ statistics are not testing the likelihood of differences; they test the size of a _ratio_. Is the variance explained by our model larger in magnitude than another variance. 

Which variance?

---

$$\Large F_{\nu_1\nu_2} = \frac{\frac{\chi^2_{\nu_1}}{\nu_1}}{\frac{\chi^2_{\nu_2}}{\nu_2}}$$
$$\Large F_{\nu_1\nu_2} = \frac{\frac{\text{Variance}_{\text{Model}}}{\nu_1}}{\frac{\text{Variance}_{\text{Residual}}}{\nu_2}}$$

$$\Large F = \frac{MS_{Model}}{MS_{residual}}$$
---

.pull-left[
The degrees of freedom for our model are 

$$DF_1 = k$$
$$DF_2 = N-k-1$$

Where *k* is the number of IV's in your model, and *N* is the sample size. ]

.pull-right[

Mean squares are calculated by taking the relevant Sums of Squares and dividing by their respective degrees of freedom.

- $SS_{\text{Model}}$ is divided by $DF_1$

- $SS_{\text{Residual}}$ is divided by $DF_2$
]

---

```{r}
anova(fit.1)
```

---

```{r,highlight.output=19}
summary(fit.1)
```

---
## Model Comparisons
- Omnibus $F$-statistic is the ratio of MSregression to MSresidual

- Test of overall significance. Does your model give a better fit to the data than a model that contains no independent variables? 

---
## Model Comparisons
- How much variance remains unexplained in our model. This "left over" variance can be contrasted with an alternative model/hypothesis. Does adding a new predictor variable help explain more variance or should we stick with the most parsimonious (simplest) model?

- Every model you report implies that you are favoring that model over an alternative model, typically the null. Taking a more formal model comparison approach allows you to be more flexible and explicit. 

---
## Full vs. Restricted Models

```{r}
fit.1 <- lm(child ~ parent, data = galton.data)
fit.0 <- lm(child ~ 1, data = galton.data)

summary(fit.0)
```

---
```{r, echo=FALSE}
summary(fit.1)
```
---
.pull-left[
```{r}
anova(fit.0)
```
]

.pull-right[
```{r}
anova(fit.1)
```

]

---
## The comparison!

```{r}
anova(fit.1, fit.0)
```

---
## Model Comparisons

- Model comparisons are redundant with nil/null hypotheses and coefficient tests right now, be they'll be more flexible down the road
- Key is to start thinking about your **implicit** alternative models
- The ultimate goal would be to create two models that represent two equally plausible theories
- A model embodies your hypothesis! It is the mathematical expression of your hypothesis!

---
## Regression coefficient

$$\Large H_{0}: \beta_{1}= 0$$
$$\Large H_{1}: \beta_{1} \neq 0$$
--

We conduct a one-sample *t*-test! What do you need to do so? 
---

## Regression coefficient
$$\Large se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-r_{xy}^2}{n-2}}}$$
$$\Large t(n-2) = \frac{b_{1}}{se_{b}}$$
---

## $SE_b$

- standard errors for the slope coefficient

- represent our uncertainty (noise) in our estimate of the regression coefficient 

- different from residual standard error/deviation (but proportional to)  

- much like previously we can take our estimate (b) and put confidence regions around it to get an estimate of what could be "possible" if we ran the study again  

---

## What does the regression coefficient test?
- Does X provide any predictive information? 

- Does X provide any explanatory power regarding the variability of Y? 

- Is the the average value the best guess (i.e., is Y bar equal to the predicted value of Y?)

- Is the regression line flat? 

- Are X and Y correlated?  

---

## Intercept

- The same but different

- More complex standard error calculation as the calculation depends on how far the X value (here zero) is away from the mean of X

    - farther from the mean, less information, thus more uncertainty 
    
---
## Confidence interval for coefficents

- Same equation as we've been working with

- Estimate plus minus 1.96*se

---
## Confidence Bands
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
set.seed(123)
px.1 <- rnorm(1000, 0, 1)
pe.1 <- rnorm(1000, 0, 1)
py.1 <- .5 + .55 * px.1 + pe.1
pd.1 <- data.frame(px.1,py.1)
px.2 <- rnorm(100, 0, 1)
pe.2 <- rnorm(100, 0, 1)
py.2 <- .5 + .55 * px.2 + pe.2
pd.2 <- data.frame(px.2,py.2)
p1 <- ggplot(pd.1, aes(x = px.1,y =  py.1)) +
    geom_point() +    
    geom_smooth(method = lm) +
   scale_x_continuous(limits = c(-3, 3))  +
  scale_y_continuous(limits = c(-3, 3))
p2 <- ggplot(pd.2, aes(x=px.2, y=py.2)) +
    geom_point() +    
    geom_smooth(method=lm) +
  scale_x_continuous(limits = c(-3, 3))  +
  scale_y_continuous(limits = c(-3, 3))
library(cowplot)
plot_grid(p1, p2, ncol=2, labels = c("N = 1000", "N = 100"))
```

---

## Confidence Bands

$$\hat{Y}\pm t_{critical} * se_{residual}*\sqrt{\frac {1}{n}+\frac{(X-\bar{X})^2}{(n-1)s_{X}^2}}$$


---
## Prediction Bands
- We are predicting and individual score, not the $\hat{Y}$ for a particular level of $X$. 
- Because there is greater variation in predicting an individual value rather than a collection of individual values (i.e., the mean) the prediction band is greater
- Combines unknown variability in 1) the estimated mean (as reflected in se of b) 2) peoples scores around  mean (residual standard error) 

$\hat{Y}\pm t_{critical} * se_{residual}*\sqrt{1+ \frac {1}{n}+\frac{(X-\bar{X})^2}{(n-1)s_{X}^2}}$

---
## Prediction Bands
```{r, warning=FALSE}
temp_var <- predict(fit.1, interval = "prediction")
new_df <- cbind(galton.data, temp_var)
pred <- ggplot(new_df, aes(y = child, x = parent)) +
  geom_point() +
  geom_smooth(method = lm, se = TRUE) +
  geom_ribbon(aes(ymin = lwr, ymax = upr),
              fill = "blue",
              alpha = 0.1) +
  theme_classic(base_size = 18)
```


---
## Prediction Bands
```{r, warning=FALSE, message=FALSE, fig.align='center', echo=F}
pred
```

---

class: inverse

### Things you should be able to do:

- Can you get an individual's predicted score & residual from an equation?
- Can you calculate b0 & b1?
- Can you take the output from R and write a regression equation, being very specific?
- Can you tell me if you have a "good" model?
- Is a coefficient meaningful? How do you know and can you calculate it?
- Can you plot a regression, confidence band, and prediction band? 

---

class: inverse
## Next time...

Partial Correlations