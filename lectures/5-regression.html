<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Univariate regression</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.27/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Univariate regression
]

---


&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 50% !important;
}

&lt;/style&gt;






## Last time

- Correlation as inferential test

- Fisher's r to z transformation

- Correlation matrices

- Interpreting effect size

---

## Today

**Regression**

- What is it? Why is it useful

- Nuts and bolts

  - Equation

  - Ordinary least squares

  - Interpretation
  
---

## GLM/Regression

GLM is a general data analytic system, meaning lots of things fall under the umbrella of regression.

* *General* - broad set of similar models; can be applied to almost any context
  * IVs can be continuous, categorical, nominal, ordinal....
* *Linear* - We try to understand our dependent variable (DV) via a linear combination predictor variables (add &amp; multiply)

---
## GLM/Regression
#### What do we get?
- effect sizes
- statistical significance
- incorporate multiple IVs
- account for intercorrelations

---

### Regression

- **Scientific** use: explaining the influence of one or more variables on some outcome. 

  
- **Prediction** use: We can develop models based on what's happened in the past to predict what will happen in the figure.

- **Adjustment**: Statistically control for known effects

  + If everyone had the same level of SES, would abuse still be associated with criminal behavior?

---

## Regression equation

What is a regression equation?

- Functional relationship

  - Ideally like a physical law `\((E = MC^2)\)`
  
  - In practice, it's never as robust as that
  
How do we uncover the relationship?

---

### How does Y vary with X?


- `\(\large E(Y|X)\)`

- "Our best guess" regardless of whether our model includes categories or continuous predictor variables

- We will evaluate our guesses based on how far away we are from the mean. But how do we come up with those guesses in the first place?

---

## Regression Equation

`$$\Large Y_i = b_{0} + b_{1}X_i +e_i$$`

`$$\Large \hat{Y_i} = b_{0} + b_{1}X_i$$`

`\(\hat{Y}\)` signifies the predicted score -- no error

The difference between the predicted and observed score is the residual ( `\(e_i\)` )

There is a different `\(e\)` value for each observation in the dataset

---

## OLS
- How do we find the regression estimates? 
- Ordinary Least Squares (OLS) estimation
- Minimizes deviations 

$$ min\sum(Y_{i}-\hat{Y})^{2} $$ 

- Other estimation procedures possible (and necessary in some cases)

---

 


![](5-regression_files/figure-html/plot1-1.png)&lt;!-- --&gt;

---

![](5-regression_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;


---

![](5-regression_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;



---

![](5-regression_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;


---

![](5-regression_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

## compare to bad fit
---

## What is error?

`$$\Large Y_i = b_{0} + b_{1}X_i +e_i$$`

`$$\Large \hat{Y_i} = b_{0} + b_{1}X_i$$`

`$$\Large Y_i = \hat{Y_i} + e_i$$`

`$$\Large e_i = Y_i - \hat{Y_i}$$`

---

## OLS

The line that yields the smallest sum of squared deviations

`$$\Large \Sigma(Y_i - \hat{Y_i})^2$$`
`$$\Large = \Sigma(Y_i - (b_0+b_{1}X_i))^2$$`
`$$\Large = \Sigma(e_i)^2$$`

--

In order to find the OLS solution, you could try many different coefficients `\((b_0 \text{ and } b_{1})\)`... or not

---
## Regression coefficient, `\(b_{1}\)`

`$$\large b_{1} = \frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \frac{s_{y}}{s_{x}}$$`

`$$\large r_{xy} = \frac{s_{xy}}{s_xs_y}$$`


*Suggested Practice: Go in `R` and see if you can prove to yourself that these equations are the same!*

---
## Standardized regression

- Regression using z-scores for Y and X
- Correlation equals standardized regression coefficient

`$$\large b_{1} = r_{xy} \frac{s_{y}}{s_{x}}$$`

$$ \large r_{xy} = b_1\frac{s_x}{s_y} $$
---
## Standardized regression

If the variance of both X and Y is equal to 1 (as in z-scores): 

`$$\large \beta_{1} = b_{1}^* = r_{xy}$$`

---

## Standardized regression equation

`$$\large Y = b_{1}^*X+e$$`

`$$\large b_{1}^* = b_{1}\frac{s_x}{s_y}$$`
--

According to this regression equation, when `\(X = 0, Y = 0\)`. Our interpretation of the coefficient is that a one-standard deviation increase in X is associated with a `\(b_{1}^*\)` standard deviation increase in Y. Our regression coefficient is equivalent to the correlation coefficient *when we have only one predictor in our model.*

---

## Estimating the intercept raw `\(b_0\)`

- Re-write equation to include `\(\bar{X}\)` &amp; `\(\bar{Y}\)`
- Intercept serves to adjust for differences in means between X and Y

`$$\hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X})$$`
- If standardized, intercept drops out
- Otherwise, intercept is where regression line crosses the y-axis at X = 0  
- When  `\(X = \bar{X}\)` the regression line goes through  `\(\bar{Y}\)`. This is true for all regressions -- line must pass through `\(\bar{X}\)`  and  `\(\bar{Y}\)`

---




``` r
galton.data &lt;- psychTools::galton
head(galton.data)
```

```
##   parent child
## 1   70.5  61.7
## 2   68.5  61.7
## 3   65.5  61.7
## 4   64.5  61.7
## 5   64.0  61.7
## 6   67.5  62.2
```

``` r
describe(galton.data, fast = T)
```

```
##        vars   n  mean   sd median  min  max range  skew kurtosis   se
## parent    1 928 68.31 1.79   68.5 64.0 73.0     9 -0.04     0.05 0.06
## child     2 928 68.09 2.52   68.2 61.7 73.7    12 -0.09    -0.35 0.08
```

``` r
cor(galton.data)
```

```
##           parent     child
## parent 1.0000000 0.4587624
## child  0.4587624 1.0000000
```

---

If we regress child height (Y) onto parents' (X):


``` r
r = cor(galton.data)[2,1]
m_parent = mean(galton.data$parent)
m_child = mean(galton.data$child)
s_parent = sd(galton.data$parent)
s_child = sd(galton.data$child)

(b1 = r*(s_child/s_parent))
```

```
## [1] 0.6462906
```

``` r
(b0 = m_child - b1*m_parent)
```

```
## [1] 23.94153
```


How will this change if we regress parent height onto child height?

---


``` r
(b1 = r*(s_child/s_parent))
```

```
## [1] 0.6462906
```

``` r
(b0 = m_child - b1*m_parent)
```

```
## [1] 23.94153
```



``` r
(b1 = r*(s_parent/s_child))
```

```
## [1] 0.3256475
```

``` r
(b0 = m_parent - b1*m_child)
```

```
## [1] 46.13535
```

---
## In `R`


``` r
fit.1 &lt;- lm(child ~ parent, data = galton.data)
summary(fit.1)
```

```
## 
## Call:
## lm(formula = child ~ parent, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.8050 -1.3661  0.0487  1.6339  5.9264 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***
## parent       0.64629    0.04114  15.711   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.239 on 926 degrees of freedom
## Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16
```

---
## Reversed


``` r
summary(lm(parent ~ child, data = galton.data))
```

```
## 
## Call:
## lm(formula = parent ~ child, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6702 -1.1702 -0.1471  1.1324  4.2722 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 46.13535    1.41225   32.67   &lt;2e-16 ***
## child        0.32565    0.02073   15.71   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.589 on 926 degrees of freedom
## Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16
```

???

**Things to discuss**

- Coefficient estimates
- Statistical tests (covered in more detail soon)

---


![](5-regression_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

---

### Data, predicted, and residuals


``` r
library(broom)
model_info = augment(fit.1)
head(model_info)
```

```
## # A tibble: 6 Ã— 8
##   child parent .fitted .resid    .hat .sigma .cooksd .std.resid
##   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;
## 1  61.7   70.5    69.5  -7.81 0.00270   2.22 0.0165       -3.49
## 2  61.7   68.5    68.2  -6.51 0.00109   2.23 0.00462      -2.91
## 3  61.7   65.5    66.3  -4.57 0.00374   2.23 0.00787      -2.05
## 4  61.7   64.5    65.6  -3.93 0.00597   2.24 0.00931      -1.76
## 5  61.7   64      65.3  -3.60 0.00735   2.24 0.00966      -1.62
## 6  62.2   67.5    67.6  -5.37 0.00130   2.23 0.00374      -2.40
```


``` r
describe(model_info)
```

```
##            vars   n  mean   sd median trimmed  mad   min   max range  skew
## child         1 928 68.09 2.52  68.20   68.12 2.97 61.70 73.70 12.00 -0.09
## parent        2 928 68.31 1.79  68.50   68.32 1.48 64.00 73.00  9.00 -0.04
## .fitted       3 928 68.09 1.16  68.21   68.10 0.96 65.30 71.12  5.82 -0.04
## .resid        4 928  0.00 2.24   0.05    0.06 2.26 -7.81  5.93 13.73 -0.24
## .hat          5 928  0.00 0.00   0.00    0.00 0.00  0.00  0.01  0.01  1.99
## .sigma        6 928  2.24 0.00   2.24    2.24 0.00  2.22  2.24  0.01 -2.39
## .cooksd       7 928  0.00 0.00   0.00    0.00 0.00  0.00  0.02  0.02  3.44
## .std.resid    8 928  0.00 1.00   0.02    0.03 1.01 -3.49  2.65  6.14 -0.24
##            kurtosis   se
## child         -0.35 0.08
## parent         0.05 0.06
## .fitted        0.05 0.04
## .resid        -0.23 0.07
## .hat           3.47 0.00
## .sigma         8.62 0.00
## .cooksd       17.34 0.00
## .std.resid    -0.23 0.03
```

???

Point out the average of the residuals is 0, just like average deviation from the mean is 0. 

---

## Residuals

- Dispersion of residuals can be thought of as what is left over in Y that is *not* explained by our model. As residuals get smaller on average, so will the SD of the residuals.

- Sigma ( `\(\sigma\)` ) is the SD of residuals. It can be thought of as how much left over in Y that we cannot explain by our model.

---


``` r
model_info %&gt;% ggplot(aes(x = parent, y = .fitted)) +
  geom_point() + geom_smooth(se = F, method = "lm") + ggtitle(expression(paste("X is related to ", hat(Y))))+
  scale_x_continuous("X") + scale_y_continuous(expression(hat(Y))) + theme_bw(base_size = 30)
```

![](5-regression_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;

---


``` r
model_info %&gt;% ggplot(aes(x = parent, y = .resid)) +
  geom_point() + geom_smooth(se = F, method = "lm") + ggtitle("X is always unrelated to e")+
  scale_x_continuous("X") + scale_y_continuous("e") + theme_bw(base_size = 30)
```

![](5-regression_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;

---


``` r
model_info %&gt;% ggplot(aes(x = child, y = .fitted)) +
  geom_point() + geom_smooth(se = F, method = "lm") + ggtitle(expression(paste("Y can be related to ", hat(Y))))+
  scale_x_continuous("Y") + scale_y_continuous(expression(hat(Y))) + theme_bw(base_size = 30)
```

![](5-regression_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;

---


``` r
model_info %&gt;% ggplot(aes(x = child, y = .resid)) +
  geom_point() + geom_smooth(se = F, method = "lm") + ggtitle("Y is sometimes related to e")+
  scale_x_continuous("Y") + scale_y_continuous("e") + theme_bw(base_size = 25)
```

![](5-regression_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;

---


``` r
model_info %&gt;% ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() + geom_smooth(se = F, method = "lm") + ggtitle(expression(paste(hat(Y), " is always unrelated to e")))+
  scale_y_continuous("e") + scale_x_continuous(expression(hat(Y))) + theme_bw(base_size = 30)
```

![](5-regression_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;

---


``` r
model_info %&gt;% rename(y = child, x = parent) %&gt;% select(x,y,.fitted,.resid) %&gt;% gather("key", "value") %&gt;%
  ggplot(aes(value, fill = key)) + geom_histogram(bins = 25) + guides(fill = F)+
  facet_wrap(~key, scales = "free") + theme_bw(base_size = 20)
```

![](5-regression_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;

---

## Residuals Summary

- Residuals are not correlated with `\(X\)` and `\(\hat{Y}\)` because those two are perfectly correlated with one another (that is, `\(r_{\text{fitted,x}} = 1\)`  )
- `\(X\)` and `\(\hat{Y}\)` represent the *same* information. We use our model ( `\(X\)` ) to make a prediction ( `\(\hat{Y}\)` ).
- No correlation between residuals with `\(X\)` and `\(\hat{Y}\)` because they are created by subtracting them out of `\(Y\)`. ( `\(\epsilon = Y - \hat{Y}\)` )
- `\(\sigma\)`  (SD of residuals) can be thought of as how much left over in `\(Y\)` after we take out all of the information our model provides. 

---

class: inverse

## Next time... 

Statistical inferences with regression
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
