<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Logistic Regression</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.7/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Logistic Regression

---




&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 65% !important;
}
&lt;/style&gt;
## Logistic Regression

Used when your DV is binary (0,1)  
    - Clinical diagnosis   
    - Disease prevalence    
    - Experiences (Yes/No)    
---

## Assumption violations
    
Violates:      
    - Correctly specified form  
    - Homoscedasticity   
    - Normality of the errors  
    
---
&lt;img src="images/Log.1.png" width="80%" /&gt;


---
&lt;img src="images/Log.2.png" width="80%" /&gt;

---
&lt;img src="images/Log.3.png" width="80%" /&gt;

---
## Need to think in terms of probabilities

- If we use OLS, we violate assumptions and have predicted values that go outside 0 &amp; 1   
- How does the predicted probability of getting a 0 or a 1 relate to our predictors?       

`$$\hat{p}_{i} \leftrightsquigarrow b_{0} + b_{1}X_{1} + b_{2}X_{2}... b_{3}X_{p}$$`

---
## Generalized linear models 

- extend the general linear model framework
- used to describe different Data Generating Processes (DGPs) other than gausian normal
- need to use if we cannot use the Gaussian normal e.g. the range of Y is restricted (e.g. binary, count) and/or the variance of Y depends on the mean, etcetera
- in other words, what is your DGP?
&lt;br&gt;


---
## Generalized linear models 
.pull-left[
- Gaussian model is 
`$$y_i \sim  N(\mu_i, \sigma)$$`

`$$\mu_i = \alpha + \beta x_i$$`
]

.pull-right[
- Binomial outcome is
`$$y_i \sim  Binomial(n, p_i)$$`

`$$f(p_i) = \alpha + \beta x_i$$`
]

---
-GLM components

1. Describe Family. What distribution is your DGP? 

2. Specify Link Function. How does your outcome relate to the predictors. 

It is not common to have an average outcome `\(\mu\)` (other than in a normal distribution) and it is not common to have parameters range from negative to positive infinity. We need something that translates our model into the parameters that describe the distribution.  

---
## Logistic regression
`$$y_i \sim  Binomial(n, p_i)$$`

`$$f(p_i) = \alpha + \beta x_i$$`
 
 Our number of trials is N, (here it is 1, technically a Bernoulli) so we are predicting the probability of y. Probabilities are bounded between zero and one. 
 
 Because our variables are not in probability units we need to "link" them via a function. The linear right side of the equation is not in the same units as the non-linear left side  
 
 Two most popular are logit and log. Others are available too: probit, negative inverse, cloglog. Typically, if you define your distribution there is a link that is recommended. 

---
## Link function for logistic

- we need to map (0,1) to `\((-\infty, \infty)\)`
- Logistic regression uses the logistic function to link the predicted probabilities to the predictors   
- Think of it as a transformation of Y-hats
`$$g(\mu) = logit(\mu) = log (\frac {\mu_{i}}{1-\mu_{i}})$$`

---
.pull-left[
`$$f(x) = \frac{1}{1+e^{-X}}$$`
]

.pull-right[
&lt;img src="images/Log.4.png" width="80%" /&gt;
 ]
 
---
`$$f(x) = \frac{1}{1+e^{-X}}$$`
`$$\hat{p} = \frac{1}{1+e^{-b_{0}+b_{1}X}}$$`
- The form of the logistic function is still nonlinear (because probabilities can only range from 0 to 1)  
- Solution is to convert probabilities into odds 

---
## odds

- Odds are defined as the probability of being a case divided by the probability of being a noncase
- Not bound between 0 and 1 
- Range from 0 to infinity
- less than one is less than 50% probability
`$$odds = \frac {\hat{p}}{1-\hat{p}}$$`

`$$probability= \frac{\hat{odds}}{1+\hat{odds}}$$`

---
## linear probability model

`$$f(x) = \frac{1}{1+e^{-X}}$$`
`$$\hat{p} = \frac{1}{1+e^{-b_{0}+b_{1}X}}$$`

`$$odds = \frac {\hat{p}}{1-\hat{p}}=e^{b_{0}+b_{1}X}$$`
`$$logit= Log(odds)=ln(\frac{\hat{p}}{1-\hat{p}}) = b_{0}+b_{1}X$$`

---
`$$logit= Log(odds)=ln(\frac{\hat{p}}{1-\hat{p}}) = b_{0}+b_{1}X$$`

-Predicted scores are not dichotomous   
-Instead of predicting probabilities directly, we are instead predicting the log of the odds.


---
## other distributions

- Exponential, which normal, gamma, binomial, Possion are apart of
- Exgaussian, beta, multi-nomial, zero inflated poisson...

---
## estimation  

- Maximum likelihood 
- OLS minimizes the errors, which also maximize R   
- In logistic regression we are not so lucky  
- Need to rely on iterative procedure, ML Estimation
- Asymptotic standard errors (approximations)  
- Interpret test statistics as z’s, not t’s  
- Wald test = chi square with 1 df = z^2 when F(1, infinity)

---
## GLM in R


```r
glm(formula, family = gaussian(link="identity"), data, weights, subset, na.action, start = NULL, etastart, mustart, offset, 
    control = glm.control(...), model = TRUE,
  method = ”glm.fit”, x = FALSE, y = TRUE, contrasts = NULL, ...)
```

- Family Argument
The family argument specifies the distribution. While link specifies link. In R, families have default links. 

---

```r
glm(y ~ X1+ X2 + X3 , family = binomial, data = dataset)

  binomial(link = "logit")
  gaussian(link = "identity")
  Gamma(link = "inverse")
  inverse.gaussian(link = "1/mu2")  
  poisson(link = "log")
```

---
## how to interpret
- b1 is the predicted change in the logit for a 1-unit change in X, holding the other predictors constant 

- For a one-unit change in X, holding other predictors constant, the odds that Y = 1 changes by `\(e^{b_{1}}\)`   

- e.g,. `\(b_{1}\)` = .4, `\(e^{.4}\)` = 1.49 

- for fitted values, need to use entire equation
`$$\hat{Y} = e^{b_{0}+b_{1}X_{1}}$$`

- turn to probabilities by: odds/(1+odds)

---


.tiny[

```r
# 1 = not premature
mortality
```

```
## # A tibble: 300 x 4
##    Intelligence_Self Intelligence_Mate premature.d NOT.premature
##                &lt;dbl&gt;             &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;
##  1                22                19 normal                  1
##  2                22                18 normal                  1
##  3                21                21 normal                  1
##  4                22                17 normal                  1
##  5                19                18 normal                  1
##  6                19                20 premature               0
##  7                16                18 normal                  1
##  8                15                11 premature               0
##  9                16                21 normal                  1
## 10                19                22 normal                  1
## # … with 290 more rows
```
]

---
.tiny[

```r
death.1 &lt;- lm(NOT.premature ~ Intelligence_Self , data = mortality)
summary(death.1)
```

```
## 
## Call:
## lm(formula = NOT.premature ~ Intelligence_Self, data = mortality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.9030  0.1084  0.1538  0.1907  0.3355 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       0.641769   0.098636   6.506 3.25e-10 ***
## Intelligence_Self 0.011357   0.005807   1.956   0.0514 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.3745 on 298 degrees of freedom
## Multiple R-squared:  0.01267,	Adjusted R-squared:  0.009362 
## F-statistic: 3.826 on 1 and 298 DF,  p-value: 0.05141
```
]

---
.tiny[

```r
death.2 &lt;- glm(NOT.premature ~ Intelligence_Self , data = mortality)
summary(death.2)
```

```
## 
## Call:
## glm(formula = NOT.premature ~ Intelligence_Self, data = mortality)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9030   0.1084   0.1538   0.1907   0.3355  
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       0.641769   0.098636   6.506 3.25e-10 ***
## Intelligence_Self 0.011357   0.005807   1.956   0.0514 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for gaussian family taken to be 0.1402466)
## 
##     Null deviance: 42.330  on 299  degrees of freedom
## Residual deviance: 41.793  on 298  degrees of freedom
## AIC: 266.05
## 
## Number of Fisher Scoring iterations: 2
```
]

---

```r
anova(death.1)
```

```
## Analysis of Variance Table
## 
## Response: NOT.premature
##                    Df Sum Sq Mean Sq F value  Pr(&gt;F)  
## Intelligence_Self   1  0.537 0.53653  3.8256 0.05141 .
## Residuals         298 41.793 0.14025                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---

```r
anova(death.2)
```

```
## Analysis of Deviance Table
## 
## Model: gaussian, link: identity
## 
## Response: NOT.premature
## 
## Terms added sequentially (first to last)
## 
## 
##                   Df Deviance Resid. Df Resid. Dev
## NULL                                299     42.330
## Intelligence_Self  1  0.53653       298     41.793
```

---
.tiny[

```r
death.3 &lt;- glm(NOT.premature ~ Intelligence_Self,
               family = binomial, data = mortality)
summary(death.3)
```

```
## 
## Call:
## glm(formula = NOT.premature ~ Intelligence_Self, family = binomial, 
##     data = mortality)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1175   0.4923   0.5716   0.6438   0.9943  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)        0.28695    0.67490   0.425   0.6707  
## Intelligence_Self  0.08012    0.04143   1.934   0.0532 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 273.53  on 299  degrees of freedom
## Residual deviance: 269.75  on 298  degrees of freedom
## AIC: 273.75
## 
## Number of Fisher Scoring iterations: 4
```
]
---

```r
exp(1)^.08012
```

```
## [1] 1.083417
```

```r
#For every 1 unit increase in Intelligence 
#the odds of living increase by 8%.

#prob = 
1.083417/(1+1.083417)
```

```
## [1] 0.5200193
```

```r
exp(1)^(.08012*20 + 0.28695)
```

```
## [1] 6.615067
```

```r
6.615067/ (1+6.615067)
```

```
## [1] 0.8686814
```

---
## probit 

.tiny[

```r
death.4 &lt;- glm(NOT.premature ~ Intelligence_Self,
        family = binomial(link = "probit"), data = mortality)
summary(death.4)
```

```
## 
## Call:
## glm(formula = NOT.premature ~ Intelligence_Self, family = binomial(link = "probit"), 
##     data = mortality)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1265   0.4889   0.5723   0.6454   0.9750  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)        0.21961    0.38376   0.572   0.5671  
## Intelligence_Self  0.04513    0.02319   1.946   0.0516 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 273.53  on 299  degrees of freedom
## Residual deviance: 269.72  on 298  degrees of freedom
## AIC: 273.72
## 
## Number of Fisher Scoring iterations: 4
```
]

---
.tiny[

```r
both &lt;- glm(NOT.premature ~ Intelligence_Mate +Intelligence_Self, 
                 family = binomial, data = mortality)
summary(both)
```

```
## 
## Call:
## glm(formula = NOT.premature ~ Intelligence_Mate + Intelligence_Self, 
##     family = binomial, data = mortality)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2470   0.4346   0.5495   0.6517   1.5110  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)        2.05138    0.99581   2.060   0.0394 * 
## Intelligence_Mate -0.12693    0.04993  -2.542   0.0110 * 
## Intelligence_Self  0.11906    0.04475   2.661   0.0078 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 273.53  on 299  degrees of freedom
## Residual deviance: 262.80  on 297  degrees of freedom
## AIC: 268.8
## 
## Number of Fisher Scoring iterations: 4
```
]

---


```r
cor(mortality$Intelligence_Self, mortality$Intelligence_Mate)
```

```
## [1] 0.3557737
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
