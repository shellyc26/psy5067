<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Multiple Regression III</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.25/header-attrs.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Multiple Regression III
]

---


&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 50% !important;
}

&lt;/style&gt;





### Last time...

* Model comparison
* Categorical predictors/dummy coding

--

### Today...

ANOVA, the long way

It's important to note that by covering dummy coding a categorical variable, we have already covered ANOVA -- there is nothing more you can learn from this technique that the omnibus test of that model will not tell you. 

---

Eye-hand coordination task while subjected to periodic 3-second bursts of 85 dB white noise played over earphones. They had participants keep a mouse pointer on a red dot that moved in a circle at a rate of 1 revolution per second. Participants performed the task until they allowed the pointer to stray from the rotating dot 10 times.  The time (in seconds) at the 10th failure was  the outcome measure.

![](images/dot.jpg)
---


The participants were randomly assigned to one of four noise conditions:

* controllable and predictable noise
* uncontrollable but predictable noise
* controllable but unpredictable noise
* uncontrollable and unpredictable noise.

When noise was *predictable*, the 3-second bursts of noise would occur regularly every 20 seconds. 

When noise was *unpredictable*, the 3-second bursts would occur randomly (although every 20 seconds on average).  

---
When noise was *uncontrollable*, participants could do nothing to prevent the noise from occurring. 

When noise was *controllable*, participants were shown a button that would prevent the noise, but they were told, "the button is a safety measure, for your protection, but we would prefer that you not use it unless absolutely necessary." No participants actually used the button.  

Why is **random assignment** important in this design?

---


```r
library(here)
rotate = read.csv(here("data/pursuit_rotor.csv"))
head(rotate)
```

```
##   ID Predict Control Group Time
## 1  1       0       1     3  504
## 2  2       0       0     1  443
## 3  3       1       1     4  292
## 4  4       0       1     3  398
## 5  5       1       1     4  119
## 6  6       0       0     1  545
```

```r
class(rotate$Group)
```

```
## [1] "integer"
```

```r
rotate$Group_lab = factor(rotate$Group, 
                          labels = c("Controllable\nPredictable", 
                                     "Uncontrollable\nPredictable",
                                     "Controllable\nUnpredictable",
                                     "Uncontrollable\nUnpredictable"))
```

---


```r
library(ggpubr)
ggboxplot(data = rotate, x = "Group_lab", y = "Time", xlab = F)
```

![](10-m_regression_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;
--
Performance degrades with either uncontrollable noise or unpredictable noise. Noise that is both uncontrollable and unpredictable is particularly disruptive.

---


```r
ggbarplot(data = rotate, x = "Group_lab", y = "Time", xlab = F, add = c("mean_ci"), fill = "purple")
```

![](10-m_regression_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

Addition of the confidence intervals indicates that the two extreme conditions are likely different from all of the other conditions.

---

&lt;table class="table" style="font-size: 24px; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Group_lab &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; N &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Mean &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SD &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Controllable
Predictable &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 47 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 542.04 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 57.11 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Uncontrollable
Predictable &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 52 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 352.85 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 67.98 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Controllable
Unpredictable &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 45 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 379.04 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 71.85 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Uncontrollable
Unpredictable &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 265.98 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 68.68 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

The groups differ in their sample sizes, which can easily occur with free random assignment.  There are advantages to equal sample sizes, so researchers often restrict random assignment to insure equal sample sizes across conditions.

Hard to tell if the variability in the four groups is homogeneous. 

---
### Hypothesis

Unlike regression, the ANOVA framework has a single hypothesis test. This is equivalent to the omnibus test of a multiple regression model.

.pull-left[
### Regression

`$$H_0: \rho^2_{Y\hat{Y}} = 0$$`
`$$H_1: \rho^2_{Y\hat{Y}} &gt; 0$$`
]

.pull-left[
### ANOVA

`$$\small H_0: \mu_1 = \mu_2 = \mu_3 = \mu_4$$`
`$$\small H_1: \text{Not that } \mu_1 = \mu_2 = \mu_3 = \mu_4$$`
This can occur in a number of ways.
]

---
## ANOVA

The total variability of all of the data, regardless of group membership, can be expressed as:

`$$\large Var(Y) = \frac{1}{N}\sum_{k=1}^G\sum_{i=1}^{N_k}(Y_{ik}-\bar{Y})^2$$`

for G groups and `\(N_k\)` participants within groups. 

---

## ANOVA

We are interested in the numerator of this variance equation, known as the **total sum of squares**:

`$$\large SS_{tot} = \sum_{k=1}^G\sum_{i=1}^{N_k}(Y_{ik}-\bar{Y})^2$$`
It's worth noting that this is just a more complicated way of expressing total sums of squares, but in a way that will be useful for thinking about how we partition sums of squares later. 

---
## ANOVA

We already know from regression that the deviation of a score from the grand mean is the sum of two independent parts. In regression these parts are the deviation of the actual score from the predicted score, and  the deviation of the predicted value from the grand mean. 

`$$\large Y_{i}-\bar{Y} = (Y_{i}-\hat{Y}_i) + (\hat{Y} - \bar{Y}_i)$$`
---


In ANOVA, this holds true, but we express these relationships by referring to each Y within a group, and instead of "predicted value" we talk about group means. So now the parts are the deviation of the score from its group mean, and the deviation of that group mean from the grand mean. Why do we substitute "group mean" for "predicted value"?

`$$\large Y_{ik}-\bar{Y} = (Y_{ik}-\bar{Y}_k) + (\bar{Y}_k - \bar{Y})$$`

Each deviation score has a within-group part and a between-group part. These separate parts can be squared and summed, giving rise to two other sums of squares. 

---
One part, the within-groups sum of squares, represents squared deviations of scores around the group means:

`$$\large SS_w = \sum_{k=1}^G\sum_{i=1}^{N_k}(Y_{ik}-\bar{Y}_k)^2$$`
The other, the between-groups sum of squares, represents deviations of the group means around the grand mean:

`$$\large SS_b = \sum_{k=1}^GN_k(\bar{Y}_{k}-\bar{Y})^2$$`

---
Sums of squares are central to ANOVA.  They are the building blocks that represent different sources of variability in a research design. They are additive, meaning that the `\(SS_{tot}\)` can be partitioned, or divided, into independent parts.

The total sum of squares is the sum of the between-groups sum of squares and the within-groups sum of squares. 

`$$\large SS_{tot} = SS_b + SS_w$$`
The relative magnitude of sums of squares, especially in more complex designs, provides a way of identifying particularly large and important sources of variability.
---

If the null hypothesis is true, then variability among the means should be consistent with the variability of the data.

`$$\hat{\sigma}^2_{\bar{Y}} = \frac{\hat{\sigma}^2_{Y}}{N}$$`
In other words, if we have an estimate of the variance of the means, we can transform that into an estimate of the variance of the scores, provided the only source of mean variability is sampling variability (the null hypothesis).

`$$N\hat{\sigma}^2_{\bar{Y}} = \hat{\sigma}^2_Y$$`
---

The `\(SS_w\)` is qualitatively giving us information that is similar to 

`$$\hat{\sigma}^2_Y$$`
The `\(SS_b\)` is qualitatively giving us information that is similar to 
`$$N\hat{\sigma}^2_{\bar{Y}}$$`

These are arrived at separately, but under the null hypothesis they should be estimates of the same thing.  The only reason that `\(SS_b\)` would be larger than expected is if there are systematic differences among the mean, perhaps created by experimental manipulations.

---

Because the sums of squares are numerators of variance estimates, we can divide each by their respective degrees of freedom to get variance estimates that, under the null hypothesis, should be approximately the same.

These variance estimates are known as mean squares:
.pull-left[
`$$\large MS_w = \frac{SS_w}{df_w}$$`
`$$\large df_w = N-G$$`
]
.pull-right[
`$$\large MS_b = \frac{SS_b}{df_b}$$`
`$$\large df_b = G-1$$`
]

---
ANOVA assumes homogeneity of group variances.  Under that assumption, the separate group variances are pooled to provide a single estimate of within-group variability.

`$$SS_w = \sum_{k=1}^G\sum_{i=1}^{N_k}(Y_{ik}-\bar{Y}_k)^2 = \sum_{k=1}^G(N_k-1)\hat{\sigma}^2_k$$`
The degrees of freedom are likewise pooled:

`$$df_w = N-G=(N_1-1)+(N_2-1)+\dots$$`

---

If data are normally distributed, then the variance is `\(\chi^2\)` distributed.  The ratio of two `\(\chi^2\)` distributed variables is `\(F\)` distributed.

In ANOVA, the ratio of the mean squares (variance estimates) is symbolized by `\(F\)` and has a sampling distribution that is `\(F\)` distributed with degrees of freedom equal to `\(df_b\)` and `\(df_w\)`.  That sampling distribution is used to determine if an obtained `\(F\)` statistic is unusual enough to reject the null hypothesis.

`$$\large F = \frac{MS_b}{MS_w}$$`
---

`$$F = \frac{MS_b}{MS_w}$$`

If the null hypothesis is true, `\(F\)` has an expected value of approximately 1.00.

If the null hypothesis is false, the numerator will be larger than the denominator because systematic, between-group differences contribute to the variance of the means, but not to the variance within groups (ideally).  If `\(F\)` is "large enough," we will reject the null hypothesis as a reasonable description of the obtained variability. 

---

`$$\large F = \frac{\hat{Q} + \hat{\sigma}^2}{\hat{\sigma}^2}$$`

`\(Q\)` is the treatment effect that, if present, increases variability among the means but does not affect the variability of scores within a group (treatment is a constant within any group).

As the treatment effect gets larger, the `\(F\)` statistic gets bigger. If it departs enough, we claim `\(F\)` to be rare or unusual. The shape of the `\(F\)` distribution is defined by its parameters, `\(df_b\)` and `\(df_w\)`.

---

![](10-m_regression_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
For 3 and 196 degrees of freedom, an `\(F\)` statistic of 2.65 or greater will indicate between-groups variability relative to within-group variability that is unusual if the null hypothesis is true.


---


```
##              Df  Sum Sq Mean Sq F value Pr(&gt;F)    
## Group_lab     3 2000252  666751   149.8 &lt;2e-16 ***
## Residuals   196  872288    4450                   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

The `\(F\)` statistic is highly unusual in the `\(F\)` distribution, assuming the null hypothesis is true.  We reject the null hypothesis.

--

What's different?


```
##              Df  Sum Sq Mean Sq F value Pr(&gt;F)    
## Group         1 1613798 1613798   253.9 &lt;2e-16 ***
## Residuals   198 1258741    6357                   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---

```
##              Df  Sum Sq Mean Sq F value Pr(&gt;F)    
## Group_lab     3 2000252  666751   149.8 &lt;2e-16 ***
## Residuals   196  872288    4450                   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
We know the means are not equal, but the particular source of the inequality is not revealed by the `\(F\)` test.

One simple way to determine what is behind the significant `\(F\)` test is to compare each condition to all other conditions.

These pairwise comparisons can quickly grow in number as the number of conditions increases.  With 4 (k) conditions, we have k(k-1)/2 = 6 possible pairwise comparisons.

---

As the number of groups in the ANOVA grows, the number of possible pairwise comparisons increases dramatically. 


![](10-m_regression_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;

---

As the number of significance tests grows, and assuming the null hypothesis is true, the probability that we will make one or more Type I errors increases.  To approximate the magnitude of the problem, we can assume that the multiple pairwise comparisons are independent. The probability that we don’t make a Type I error for just one test is:

`$$P(\text{No Type I}, 1 \text{ test}) = 1-\alpha$$`
The probability that we don't make a Type I error for two tests is (note the reason we assume independence):

`$$P(\text{No Type I}, 2 \text{ test}) = (1-\alpha)(1-\alpha)$$`
---
For C tests, the probability that we make **no** Type I errors is

`$$P(\text{No Type I}, C \text{ tests}) = (1-\alpha)^C$$`
We can then use the following to calculate the probability that we make one or more Type I errors in a collection of C independent tests.

`$$P(\text{At least one Type I}, C \text{ tests}) = 1-(1-\alpha)^C$$`
As the number of comparisons grows, the inflation of the Type I error rate can be substantial.

**What is this called?**

---

Need "correction" procedures. Many exist.

![](10-m_regression_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;

---

Multiple comparisons, each tested with `\(\alpha_{per-test}\)`, increases the family-wise `\(\alpha\)` level. 

`$$\alpha_{family-wise} = 1 - (1-\alpha_{per-test})^C$$`
Šidák showed that the family-wise `\(\alpha\)` could be controlled to a desired level (e.g., .05) by changing the `\(\alpha_{per-test}\)` to:

`$$\alpha_{per-wise} = 1 - (1-\alpha_{family-wise})^{\frac{1}{C}}$$`
Bonferroni (and Dunn, and others) suggested this simple approximation: 

`$$\alpha_{per-test} = \frac{\alpha_{family-wise}}{C}$$`

This is typically called the Bonferroni correction and is very often used even though better alternatives are available. 

---


```r
library(rstatix)

rotate %&gt;% 
  pairwise_t_test(Time ~ Group,
                  p.adjust.method = "bonferroni")
```

```
## # A tibble: 6 × 9
##   .y.   group1 group2    n1    n2        p p.signif    p.adj
## * &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;
## 1 Time  1      2         47    52 1.33e-31 ****     7.99e-31
## 2 Time  1      3         47    45 2.27e-24 ****     1.36e-23
## 3 Time  2      3         52    45 5.52e- 2 ns       3.31e- 1
## 4 Time  1      4         47    56 7.93e-52 ****     4.76e-51
## 5 Time  2      4         52    56 1.53e-10 ****     9.2 e-10
## 6 Time  3      4         45    56 5.92e-15 ****     3.55e-14
## # ℹ 1 more variable: p.adj.signif &lt;chr&gt;
```

---

The Bonferroni procedure is conservative. The most common other one is the Holm procedure.

The Holm procedure does not make a constant adjustment to each per-test `\(\alpha\)`. Instead it makes adjustments in stages depending on the relative size of each pairwise p-value.

---

## Holm Procedure

1. Rank order the p-values from largest to smallest.
2. Start with the smallest p-value. Multiply it by its rank.
3. Go to the next smallest p-value. Multiply it by its rank. If the result is larger than the previous step, keep it. Otherwise replace with the previous step adjusted p-value.
4.  Repeat Step 3 for the remaining p-values.
5. Judge significance of each new p-value against `\(\alpha = .05\)`.

---
## Holm Procedure

```r
hp = rotate %&gt;% 
  pairwise_t_test(Time ~ Group,
                  p.adjust.method = "holm") %&gt;% 
  arrange(p) %&gt;% 
  mutate(Rank = 6:1, rXp = Rank*p)

hp[,c(2,3,6:11)]
```

```
## # A tibble: 6 × 8
##   group1 group2        p p.signif    p.adj p.adj.signif  Rank
##   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;
## 1 1      4      7.93e-52 ****     4.76e-51 ****             6
## 2 1      2      1.33e-31 ****     6.66e-31 ****             5
## 3 1      3      2.27e-24 ****     9.07e-24 ****             4
## 4 3      4      5.92e-15 ****     1.78e-14 ****             3
## 5 2      4      1.53e-10 ****     3.07e-10 ****             2
## 6 2      3      5.52e- 2 ns       5.52e- 2 ns               1
## # ℹ 1 more variable: rXp &lt;dbl&gt;
```
---


```
## # A tibble: 6 × 8
##   group1 group2        p p.signif    p.adj p.adj.signif  Rank
##   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt;
## 1 1      4      7.93e-52 ****     4.76e-51 ****             6
## 2 1      2      1.33e-31 ****     6.66e-31 ****             5
## 3 1      3      2.27e-24 ****     9.07e-24 ****             4
## 4 3      4      5.92e-15 ****     1.78e-14 ****             3
## 5 2      4      1.53e-10 ****     3.07e-10 ****             2
## 6 2      3      5.52e- 2 ns       5.52e- 2 ns               1
## # ℹ 1 more variable: rXp &lt;dbl&gt;
```

A single adjusted per-test a would be used with the Bonferroni procedure: .05/6 = .0083. We would compare each original p-value to this new criterion and claim as significant only those that are less. Or, we could multiply the original p-values by 6 and judge them against a =.05.

---

class: inverse

## Next time...

- Exam 1!


(we will cover remaining slides if time)
---

### Contrasts

You can change which group is the reference:

```r
rotate$Group = factor(rotate$Group)
contrasts(rotate$Group) = contr.treatment(n = 4, base = 3)
coef(summary(lm(Time~Group, data = rotate)))
```

```
##               Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)  379.04444   9.944788 38.114882 1.388406e-92
## Group1       162.99811  13.913633 11.714993 2.267662e-24
## Group2       -26.19829  13.582501 -1.928827 5.519687e-02
## Group4      -113.06230  13.355564 -8.465558 5.923855e-15
```

You're not restricted to simple dummy coding. Choosing another variant of contrast codes allows you to test more specific hypotheses. There are some common coding schemes.
---
For example, **deviation coding** (aka **"effect coding"**):


```r
contr.sum(4)
```

```
##   [,1] [,2] [,3]
## 1    1    0    0
## 2    0    1    0
## 3    0    0    1
## 4   -1   -1   -1
```


```r
contrasts(rotate$Group) = contr.sum(n = 4)
coef(summary(lm(Time~Group, data = rotate))) %&gt;% kable(., digits = c(2,2,2,3), format = "html") %&gt;% kable_styling(font_size = 13)
```

&lt;table class="table" style="font-size: 13px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Std. Error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; t value &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Pr(&amp;gt;|t|) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 384.98 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.73 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 81.31 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Group1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 157.06 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.35 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18.80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Group2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -32.13 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.08 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -3.98 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Group3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -5.93 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.48 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.70 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.485 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

You can create a contrast matrix that tests any number of hypotheses, like whether groups 1 and 3 are different from group 4. Rules:

1. The sum of the weights across all groups for each code variable must equal 0. (Sum down the column = 0).

2. The sum of the product of each pair of code variables, `\(C_1C_2,\)` must equal 0. (This is challenging when your groups are unequal sizes).

3. The difference between the value of the set of + weights and the set of neg weights should equal 1 for each column.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
