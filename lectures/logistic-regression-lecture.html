<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Logistic Regression</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.10/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rladies-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Logistic Regression

---




&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 65% !important;
}
&lt;/style&gt;

## This whole semester...

- We've had continuous IVs (regression)

- We've had categoical IVs (factorial ANOVA)

- We've had a mixture of continuous/categorical IVs (more regression)

--
- But we've never had a DV variable that is categorical... in comes Logistic Regression

---

## Linear Regression

Let's take a step back and think about linear regression for a minute:

-  When we have a continuous DV, we can...
  -   Calculate the R^2 and determine if our IVs &amp; DV are correlated  (large value implying = large effect)
  -   Calculate a p-value to determine if R^2 is statistically significant  
  -   Use the line/slope of a linear regression to make a calculated prediction of *y* given *x*
  -   Use covariates to model *y* using *x* and these other variables using multiple regression (and compare models!)

---

## Logistic Regression

Logistic Regression does something entirely different...


When we have a categorical DV, we are trying to see if something is *TRUE* or *FALSE* rather than trying to understand or predict something on a continuous scale.


---

## Logistic Regression

Used when your DV is binary (0,1)  
- Clinical diagnosis   
- Disease prevalence    
- Experiences (Yes/No)
- Correct/Incorrect

--

The mean of the distribution is the same as the proportion of 1's in the distribution. 
- Out of 100 people, 27 have PTSD (1) and 73 do not have PTSD (0)
- The mean of the distribution is .27
- Probability of getting a 1 is .27

---

.left-column[ 
If we plotted these points, they are either 1's or 0's. Looks like this...
]
&lt;img src="images/Log.1.png" width="75%" /&gt;


---
.left-column[
If we plotted our OLS regression line, we get this. But it doesn't make sense! Predicted values can go above 1 or below 0...yikes
]
&lt;img src="images/Log.2.png" width="75%" /&gt;

---
.left-column[
Instead, we fit this non-linear function. None of our data fall on the line!   
]
&lt;img src="images/Log.3.png" width="75%" /&gt;

---

## Where is the line??

Logistic Regression, rather than a straight, fitted line like linear regression, logistic regression fits an S-shaped logistic function.

This curved line can tell us the probability that something will be "1" given *X*

---

## Model Complexity

...and just like linear regression, we can make simple models (a relationship between *x* and *y*, with *y* being a binary variable) or more complicated models (with control variables or covariates).

Unlike linear regression, it isn't as easy to just compare a more complex model to a simple model (we will talk more about this later). Rather, we will want to do what is called a "Wald's Test" to find out if our prediction is significantly different from 0 (again, more on this later!).

---

## Assumption violations

When our outcome is binary, we violate OLS regression assumptions
    
Violates:      
- Correctly specified form  (not linear)
- Homoscedasticity (as P approaches 1 or 0, variance approaches 0)
- Normality of the errors (lol nope)


---

## Need to think in terms of probabilities

- If we use OLS, we violate assumptions and have predicted values that go outside 0 &amp; 1   
- How does the predicted probability of getting a 0 or a 1 relate to our predictors?       

`$$\hat{p}_{i} \leftrightsquigarrow b_{0} + b_{1}X_{1} + b_{2}X_{2}... b_{3}X_{p}$$`

---
## Generalized linear models 

- Extends the general linear model framework

- Used to describe different Data Generating Processes (DGPs) other than Gaussian normal

- Need to use if we cannot use the Gaussian normal e.g. the range of Y is restricted (e.g. binary, count) and/or the variance of Y depends on the mean, etc...

- In other words, what is your DGP?

---
## Generalized linear models 
.pull-left[
- Gaussian model is 
`$$y_i \sim  N(\mu_i, \sigma)$$`

`$$\mu_i = \alpha + \beta x_i$$`
- The parameters of the normal distribution are the mean μ and the standard deviation σ (or the variance σ2)]

.pull-right[
- Binomial outcome is
`$$y_i \sim  Binomial(n, p_i)$$`

`$$f(p_i) = \alpha + \beta x_i$$`
- n stands for the number of times the experiment runs.
- p represents the probability of one specific outcome.]

---
## GLM components

It is not common to have an average outcome `\(\mu\)` (other than in a normal distribution) and it is not common to have parameters range from negative to positive infinity. We need something that translates our model into the parameters that describe the distribution.  

---
## Logistic regression

.pull-left[
`$$y_i \sim  Binomial(n, p_i)$$`

`$$f(p_i) = \alpha + \beta x_i$$`
 
Our number of trials is `\(N\)`, so we are predicting the probability of `\(y\)`. Probabilities are bounded between zero and one. 
]

.pull-right[
Because our variables are not in probability units we need to "link" them via a function. The linear right side of the equation is not in the same units as the non-linear left side. 
 
Two most popular are logit and log. Others are available too, such as probit.
]

---
## Link function for logistic

- We need to map (0,1) to `\((-\infty, \infty)\)`

- Logistic regression uses the logistic function to link the predicted probabilities to the predictors   

- Think of it as a transformation of `\(\hat{Y}\)`s

---
## Sigmoid function

.pull-left[
`$$f(x) = \frac{1}{1+e^{-X}}$$`

... where `\(e\)` is Euler's number



```r
exp(1)
```

```
## [1] 2.718282
```
]


.pull-right[
&lt;img src="images/Log.4.png" width="80%" /&gt;

The y-axis is the "rolling mean" of the DV (or the proportion of 1's). This logistic curve relates `\(X\)` (the IV) to our `\(P(\bar{Y})\)`
]

---
## Sigmoid function (cont.)

For the Sigmoid function, as *x* approaches `\(\infty\)`, it reaches a natural limit at 1.

As *x* approaches `\(-\infty\)`, it reaches a natural limit of 0

Interestingly, when *x* is 0, the Sigmoid function settles at 0.5.

This all keeps the limits within the 0, 1 range.

---
`$$f(x) = \frac{1}{1+e^{-X}}$$`
`$$\hat{p} = \frac{1}{1+e^{-b_{0}+b_{1}X}}$$`
- The form of the logistic function is still nonlinear (because probabilities can only range from 0 to 1)  

- Since it's nonlinear, `\(b\)` can't be interpreted as easily as we've been doing with OLS regression.

- So in order to interpret our model parameter, we need to conver to **odds**

???
where P is the probability of a 1 (the proportion of 1s, the mean of Y), e is the base of the natural logarithm (about 2.718) and a and b are the parameters of the model. The value of a yields P when X is zero, and b adjusts how quickly the probability changes with changing X a single unit (we can have standardized and unstandardized b weights in logistic regression, just as in ordinary linear regression). Because the relation between X and P is nonlinear, b does not have a straightforward interpretation in this model as it does in ordinary linear regression

---
## Odds

- Odds are defined as the probability of being a case divided by the probability of being a noncase
- Not bound between 0 and 1 
- Range from 0 to infinity
- Less than one is less than 50% probability
`$$odds = \frac {\hat{p}}{1-\hat{p}}$$`

`$$probability= \frac{\hat{odds}}{1+\hat{odds}}$$`
---
## Odds versus Probability

Put simply:

- Odds are the ratio of something happening over something not happening.
- Probability is the ratio of something happening over everything that could happen.

The odds being so asymmetrical (unlike probability) make it so difficult to compare the odds of being 0 versus the odds of being 1. So we use something called log odds.

---
## Log Odds

Taking the log of the odds solves this problem by making everything symetrical!

Odds are 1 to 6:

- log(1/6) = log(0.17) = -1.79

Odds are 6 to 1:

- log(6/1) = log(6) = 1.79


---
## Linear Probability Model

`$$f(x) = \frac{1}{1+e^{-X}}$$`
`$$\hat{p} = \frac{1}{1+e^{-b_{0}+b_{1}X}}$$`

`$$odds = \frac {\hat{p}}{1-\hat{p}}=e^{b_{0}+b_{1}X}$$`
`$$logit= Log(odds)=ln(\frac{\hat{p}}{1-\hat{p}}) = b_{0}+b_{1}X$$`

---
## Logit
`$$logit= Log(odds)=ln(\frac{\hat{p}}{1-\hat{p}}) = b_{0}+b_{1}X$$`
- DV is a logit, the natural log of odds

- Predicted scores are not dichotomous   

- Instead of predicting probabilities directly, we are instead predicting the log of the odds.

- The regression we are used to is not predicting `\(\hat{Y}\)`, because we're predicting (or relating our IV) to *logits*

---

## Now what?

- In order to get the probability of a value of X being 1 or 0 based on some model parameters ( `\(b_0\)`, `\(b_1\)` etc. ), we rearrange our equation so that we actually get the predicted logit (log of odds) given our model parameters. 

- How the hell do you interpret a logit? Not easily...

- Convert back into odds

- Convert your odds back into probabilities

---
## Estimation with Maximum Likelihood

- OLS minimizes the errors ( `\(SS_{res}\)`), which maximizes ( `\(SS_{reg}\)`)   
- In logistic regression we are not so lucky  

- Need to rely on iterative procedure, ML Estimation:

  - Pick parameters of your model ( `\(b_0\)`, `\(b_1\)` etc. ), and calculate the *likelihood* of the data, given those parameters. We do this iteratively until we find the best parameters -- the ones that *maximize* the *likelihood* of your data. 

---
## Estimation with Maximum Likelihood

More simply, we can think about using the distribution of the data to find the location that maximizes the likelihood of observing the variable that we measured. We are essentially trying to find the optimal value for the mean (or standard deviation) for a distribution giving our observed data.

We are talking here about the mean of the distribution, not the mean of the data. (However, in a normal distribution, these are the same thing.)

---
## Estimation with Maximum Likelihood

- So we get model parameters, estimated via MLE (instead of OLS)

- Same but different:

  - Asymptotic standard errors (an approximation to the standard error)  

  - Interpret test statistics as `\(z\)`’s, not `\(t\)`’s  

  - No `\(t\)`-tests; instead a Wald test = `\(\chi^2\)` test with 1 df = `\((\frac{coef}{se})^2\)`

???
asymptotic se's are just approximations since we finite-sample dist of the estimator isn't known

---
## GLM in R

.pull-left[

```r
glm(formula,
*   family = gaussian(link="identity"),
    data,
    weights,
    subset,
    na.action,
    start = NULL,
    etastart,
    mustart,
    offset, 
    control = glm.control(...),
    model = TRUE,
    method = ”glm.fit”,
    x = FALSE, 
    y = TRUE,
    contrasts = NULL, ...)
```
]

.pull-right[
The `family` argument specifies the distribution. In R, families have default links. 

![](images/glmfamily.png)
]

---
## GLM in R

.pull-left[

```r
*glm(y ~ X1+ X2 + X3 ,
    family = binomial,
    data = dataset)
```
]

.pull-right[
Specify the model like you would with `lm`
]

---
## GLM in R

.pull-left[

```r
glm(y ~ X1+ X2 + X3 , 
*   family = binomial,
    data = dataset)
```
]

.pull-right[

Specify the distribution you're working with. When binary outcomes, we'll use the binomial. 

]

---
## GLM in R

.pull-left[

```r
glm(y ~ X1+ X2 + X3 , 
    family = binomial, 
*   data = dataset)
```
]

.pull-right[

Specify your dataset.

]

---
## How to interpret
- `\(b_1\)` is the predicted change in the logit for a 1-unit change in X, holding the other predictors constant 

- For a 1-unit change in X, holding other predictors constant, the odds that Y = 1 changes by `\(e^{b_{1}}\)`   

  - e.g,. `\(b_{1}\)` = .4, `\(e^{.4}\)` = 1.49 

- For fitted values, need to use entire equation
`\(\hat{Y} = e^{b_{0}+b_{1}X_{1}}\)`

- Turn to probabilities by: `\(\frac{\text{odds}}{(1 + \text{odds})}\)`

---
## Example




```r
# 1 = not premature
mortality
```

```
## # A tibble: 300 × 4
##    Intelligence_Self Intelligence_Mate premature.d NOT.premature
##                &lt;dbl&gt;             &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;
##  1                22                19 normal                  1
##  2                22                18 normal                  1
##  3                21                21 normal                  1
##  4                22                17 normal                  1
##  5                19                18 normal                  1
##  6                19                20 premature               0
##  7                16                18 normal                  1
##  8                15                11 premature               0
##  9                16                21 normal                  1
## 10                19                22 normal                  1
## # … with 290 more rows
```

---


```r
death.1 &lt;- lm(NOT.premature ~ Intelligence_Self , data = mortality)
summary(death.1)
```

```
## 
## Call:
## lm(formula = NOT.premature ~ Intelligence_Self, data = mortality)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.9030  0.1084  0.1538  0.1907  0.3355 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       0.641769   0.098636   6.506 3.25e-10 ***
## Intelligence_Self 0.011357   0.005807   1.956   0.0514 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.3745 on 298 degrees of freedom
## Multiple R-squared:  0.01267,	Adjusted R-squared:  0.009362 
## F-statistic: 3.826 on 1 and 298 DF,  p-value: 0.05141
```


---


```r
death.2 &lt;- glm(NOT.premature ~ Intelligence_Self , data = mortality)
summary(death.2)
```

```
## 
## Call:
## glm(formula = NOT.premature ~ Intelligence_Self, data = mortality)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9030   0.1084   0.1538   0.1907   0.3355  
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       0.641769   0.098636   6.506 3.25e-10 ***
## Intelligence_Self 0.011357   0.005807   1.956   0.0514 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for gaussian family taken to be 0.1402466)
## 
##     Null deviance: 42.330  on 299  degrees of freedom
## Residual deviance: 41.793  on 298  degrees of freedom
## AIC: 266.05
## 
## Number of Fisher Scoring iterations: 2
```


---

```r
anova(death.1)
```

```
## Analysis of Variance Table
## 
## Response: NOT.premature
##                    Df Sum Sq Mean Sq F value  Pr(&gt;F)  
## Intelligence_Self   1  0.537 0.53653  3.8256 0.05141 .
## Residuals         298 41.793 0.14025                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---

```r
anova(death.2)
```

```
## Analysis of Deviance Table
## 
## Model: gaussian, link: identity
## 
## Response: NOT.premature
## 
## Terms added sequentially (first to last)
## 
## 
##                   Df Deviance Resid. Df Resid. Dev
## NULL                                299     42.330
## Intelligence_Self  1  0.53653       298     41.793
```

---


```r
death.3 &lt;- glm(NOT.premature ~ Intelligence_Self,
               family = binomial, data = mortality)
summary(death.3)
```

```
## 
## Call:
## glm(formula = NOT.premature ~ Intelligence_Self, family = binomial, 
##     data = mortality)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1175   0.4923   0.5716   0.6438   0.9943  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)        0.28695    0.67490   0.425   0.6707  
## Intelligence_Self  0.08012    0.04143   1.934   0.0532 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 273.53  on 299  degrees of freedom
## Residual deviance: 269.75  on 298  degrees of freedom
## AIC: 273.75
## 
## Number of Fisher Scoring iterations: 4
```

---
.pull-left[
For a 1-unit change in X, holding other predictors constant, the odds that Y = 1 changes by `\(e^{b_{1}}\)`


```r
exp(1)^.08012
```

```
## [1] 1.083417
```

*For every 1-unit increase in Intelligence, the odds of living increase by 8%*
]

--

.pull-right[
Now convert back into probabilities `\(\frac{\text{odds}}{(1 + \text{odds})}\)`


```r
1.083417 / (1+1.083417)
```

```
## [1] 0.5200193
```


*For every 1-unit increase in Intelligence, the probability of living increases by .52* 

Probability greater than .5 indicates success is more likely than failure.
]
---

## Specific Values?

What if you want the probability of being a premature death for a given level of Intelligence? (Now that we've run our model and have parameters...)

For fitted values, need to use entire equation
`\(\hat{Y} = e^{b_{0}+b_{1}X_{1}}\)`


```r
# get fitted value with a given value of X (here 20)
exp(1)^(0.28695 + (.08012*20))
```

```
## [1] 6.615067
```

```r
# now get odds
6.615067 / (1+6.615067)
```

```
## [1] 0.8686814
```

---
## Probit 

We can have different link functions. When your response variable (DV) is truly binary -- the data generating process generates legit binary data -- logit is your pick.

--

What if your response variable is binary, but the underlying construct you are trying to measure is likely Gaussian? Ex: depressed vs. not depressed. But the underlying latent construct is continuous. More appropriate then is the **probit** link function.

[Stack exchange thread if you're going down this route](https://stats.stackexchange.com/questions/20523/difference-between-logit-and-probit-models)
---
## Probit



```r
death.4 &lt;- glm(NOT.premature ~ Intelligence_Self,
        family = binomial(link = "probit"), data = mortality)
summary(death.4)
```

```
## 
## Call:
## glm(formula = NOT.premature ~ Intelligence_Self, family = binomial(link = "probit"), 
##     data = mortality)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1265   0.4889   0.5723   0.6454   0.9750  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)        0.21961    0.38376   0.572   0.5671  
## Intelligence_Self  0.04513    0.02319   1.946   0.0516 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 273.53  on 299  degrees of freedom
## Residual deviance: 269.72  on 298  degrees of freedom
## AIC: 273.72
## 
## Number of Fisher Scoring iterations: 4
```


---
class:inverse, middle, center

# Thank you to AJ and Mary for wonderful lectures!

---
class:inverse

## 4 more to go... 

- 3 talks on Machine Learning principles
- 1 review session
- 1 last exam!


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
