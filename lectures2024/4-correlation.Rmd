---
title: 'Correlations'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["xaringan-themer.css", "my-theme.css"]
    incremental: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

<style type="text/css">
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 50% !important;
}

</style>


```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(base_color = "#17139C",link_color = "#DD3E3E")

```

```{r, echo = F, warning = F, message = F}
library(psych)
library(tidyverse)
```

## Last time/this time

- Looked at glm models and showed how they are the same as our old friends, *t*-tests and ANOVAs, just with categorical predictors

- Moving toward continuous predictor models
---
## Relationships

- What is the relationship between IV and DV?

- Measuring relationships depend on type of measurement

- You have primarily been working with categorical IVs (*t*-test, chi-square)

---
## Scatter Plot with best fit line
```{r,echo=FALSE, message=FALSE, cache=TRUE, warning=FALSE}
library(psych)
library(psychTools)
library(tidyverse)
library(RColorBrewer)
galton.data <- galton
purples = brewer.pal(n = 3, name = "Purples")
```

```{r, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
ggplot(galton.data, aes(x=parent, y=child)) +
    geom_jitter() +    
    geom_smooth(method=lm,   # Add linear regression line
                se=FALSE) +     # Don't add shaded confidence region
    labs(x = "parent height", y = "child height")
```

---
## Review of Dispersion

Variation (sum of squares)

$$SS = {\sum{(x-\bar{x})^2}}$$
$$SS = {\sum{(x-\mu)^2}}$$
---

## Review of Dispersion
Variance

$$\large s^{2} = {\frac{\sum{(x-\bar{x})^2}}{N-1}}$$

$$\large\sigma^{2} = {\frac{\sum{(x-\mu)^2}}{N}}$$

---

## Review of Dispersion

Standard Deviation

$$\large s = \sqrt{\frac{\sum{(x-\bar{x})^2}}{N-1}}$$

$$\large\sigma = \sqrt{\frac{\sum{(x-\mu)^2}}{N}}$$

---

## Review of Dispersion

Formula for standard error of the mean?

--

$$\sigma_M = \frac{\sigma}{\sqrt{N}}$$

$$\sigma_M = \frac{\hat{s}}{\sqrt{N}}$$

---
## Associations

- i.e., relationships
- to look at continuous variable associations we need to think in terms of how variables relate to one another

---
## Associations

Covariation (cross products)

**Sample:**

$$\large SS = {\sum{(x-\bar{x})(y-\bar{y})}}$$

**Population:**

$$SS = {\sum{{(x-\mu_{x}})(y-\mu_{y})}}$$

---
## Associations

**Sample:**

$$\large cov_{xy} = {\frac{\sum{(x-\bar{x})(y-\bar{y})}}{N-1}}$$

**Population:**

$$\large \sigma_{xy}^{2} = {\frac{\sum{(x-\mu_{x})(y-\mu_{y})}}{N}}$$


> What are some issues that may arise when comparing covariances?

---
## Associations

**Sample:**

$$\large r_{xy} = {\frac{\sum({z_{x}z_{y})}}{N}}$$

**Population:**

$$\large \rho_{xy} = {\frac{cov(X,Y)}{\sigma_{x}\sigma_{y}}}$$


> Many other formulas exist for specific types of data

---

## Correlations


- How much two variables are linearly related

- -1 to 1

- Invariant to changes in mean or scaling

- Most common (and basic) effect size measure

- Will use to build our regression model

---

## Conceptually 

Ways to think about a correlation:

* How two vectors of numbers co-relate

* Product of z-scores
  
  + Mathematically, it is
  
* The average squared distance between two vectors in the same space

---

## Correlations

.pull-left[
```{r, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
ggplot(galton.data, aes(x=parent, y=child)) +
    geom_jitter(alpha = .4) +
    geom_smooth(method=lm,   # Add linear regression line
                se=FALSE) +     # Don't add shaded confidence region
    labs(x = "parent height", y = "child height") +
  theme_bw(base_size = 20)
```
]

.pull-right[
- This will become our regression line. Right now, it is our correlation line. At this point **they are the same!**
- You can do a lot of things with just correlations.
]

---

## Statistical test

Hypothesis testing

$$\large H_{0}: \rho_{xy} = 0$$

$$\large H_{A}: \rho_{xy} \neq 0$$

Assumes:
- Observations are independent
- Symmetric bivariate distribution (joint probability distribution)

---

### Population

```{r, echo=FALSE,cache=TRUE}
mu1<-0 # setting the expected value of x1
mu2<-0 # setting the expected value of x2
s11<-4 # setting the variance of x1
s12<-2 # setting the covariance between x1 and x2
s22<-5 # setting the variance of x2
rho<-0.1 # setting the correlation coefficient between x1 and x2
x1<-seq(-10,10,length=41) # generating the vector series x1
x2<-x1 # copying x1 to x2
f<-function(x1,x2)
{
term1<-1/(2*pi*sqrt(s11*s22*(1-rho^2)))
term2<--1/(2*(1-rho^2))
term3<-(x1-mu1)^2/s11
term4<-(x2-mu2)^2/s22
term5<--2*rho*((x1-mu1)*(x2-mu2))/(sqrt(s11)*sqrt(s22))
term1*exp(term2*(term3+term4-term5))
} # setting up the function of the multivariate normal density
#
z<-outer(x1,x2,f) # calculating the density values
#
persp(x1, x2, z,
main="Joint Probability Distribution", sub=expression(italic(f)~(bold(x))==frac(1,2~pi~sqrt(sigma[11]~ sigma[22]~(1-rho^2)))~phantom(0)^bold(.)~exp~bgroup("{", list(-frac(1,2(1-rho^2)),
bgroup("[", frac((x[1]~-~mu[1])^2, sigma[11])~-~2~rho~frac(x[1]~-~mu[1], sqrt(sigma[11]))~ frac(x[2]~-~mu[2],sqrt(sigma[22]))~+~ frac((x[2]~-~mu[2])^2, sigma[22]),"]")),"}")),
col= purples[3],
theta=30, phi=20,
r=50,
d=0.1,
expand=0.5,
ltheta=90, lphi=180,
shade=0.75,
ticktype="detailed",
nticks=5)

# produces the 3-D plot
mtext(expression(list(mu[1]==0,mu[2]==0,sigma[11]==4,sigma[22]==5,sigma[12 ]==2,rho==0.1)), side=3)
# adding a text line to the graph
```

---

### Sample

```{r, echo = F, warning=FALSE, message=FALSE}
sigma <- matrix( c( 4, 2,
                    2, 5 ), 2, 2 )  # covariance matrix

sample = BDgraph::rmvnorm(n = 30, mean = c(0,0), sigma = sigma)

sample %>% as.data.frame() %>%
  ggplot(aes(x = V1, y = V2)) +
  geom_smooth(method = "lm", se = F, color = purples[2])+
  geom_point(color = purples[3]) +
  scale_x_continuous("X1") + 
  scale_y_continuous("X2")+
  theme_bw(base_size = 20)
```

---

## Sampling distribution?

The sampling distribution we use depends on our null hypothesis.

--

If our null hypothesis is that $(\rho = 0)$ , then we can use a ***t*-distribution** to estimate the statistical significance of a correlation. 



---
## Test Statistic
Signal divided by noise

$$\large t = {\frac{r}{SE_{r}}}$$
--
.pull-left[
$$\large SE_r = \sqrt{\frac{1-r^2}{N-2}}$$

$$\large t = {\frac{r}{\sqrt{\frac{1-r^{2}}{N-2}}}}$$
]

.pull-right[
$$\large DF = N-2$$
]

---
## Power calculations
What sample size do you need in order to have enough power to detect a **.1** correlation? 
```{r, warning = FALSE, message = FALSE}
library(pwr)
pwr.r.test(n = , r = .1, sig.level = .05 , power = .8)
```

---
## Power calculations
What sample size do you need in order to have enough power to detect a **.3** correlation? 
```{r, warning = FALSE, message = FALSE}
library(pwr)
pwr.r.test(n = , r = .3, sig.level = .05 , power = .8)
```
---
## Power calculations
- But what is your confidence?

- N = 84 gives you CI[.09, .48]

- Schönbrodt & Perugini (2013) suggest correlations 'stabilize' at 250+ regardless of effect size



---
## Fisher's r to z' transformation
.left-column[
If we want to make calculations based on $\rho \neq 0$ then we will run into a skewed sampling distribution.
]

```{r, echo = F}
r_sampling = function(x, r, n){
  z = fisherz(r)
  se = 1/(sqrt(n-3))
  x_z = fisherz(x)
  density = dnorm(x_z, mean = z, sd = se)
  return(density)
}


cor_75 = ggplot(data.frame(x = seq(-.99, .99)), aes(x)) +
  stat_function(fun = function(x) r_sampling(x, r = .75, 
                                             n = 30),
                geom = "area", fill = purples[3]) +
  scale_x_continuous(limits = c(-.99, .99)) +
  labs(subtitle = "r = .75, n = 30") +
  theme_bw(base_size = 20) +
  theme(plot.subtitle = element_text(size = 12))

cor_32 = ggplot(data.frame(x = seq(-.99, .99)), aes(x)) +
  stat_function(fun = function(x) r_sampling(x, r = .32, 
                                             n = 30),
                geom = "area", fill = purples[2]) +
  scale_x_continuous(limits = c(-.99, .99)) +
  labs(subtitle = "r = .32, n = 30")+
  theme_bw(base_size = 20) +
  theme(plot.subtitle = element_text(size = 12))


cor_n85 = ggplot(data.frame(x = seq(-.99, .99)), aes(x)) +
  stat_function(fun = function(x) r_sampling(x, r = -.85, 
                                             n = 30),
                geom = "area", fill = purples[3]) +
  scale_x_continuous(limits = c(-.99, .99)) +
  labs(subtitle = "r = -.85, n = 30")+
  theme_bw(base_size = 20) +
  theme(plot.subtitle = element_text(size = 12))

cor_75b = ggplot(data.frame(x = seq(-.99, .99)), aes(x)) +
  stat_function(fun = function(x) r_sampling(x, r = .75, 
                                             n = 150),
                geom = "area", fill = purples[3]) +
  scale_x_continuous(limits = c(-.99, .99)) +
  labs(subtitle = "r = .75, n = 150") +
  theme_bw(base_size = 20) +
  theme(plot.subtitle = element_text(size = 12))

cor_32b = ggplot(data.frame(x = seq(-.99, .99)), aes(x)) +
  stat_function(fun = function(x) r_sampling(x, r = .32, 
                                             n = 150),
                geom = "area", fill = purples[2]) +
  scale_x_continuous(limits = c(-.99, .99)) +
  labs(subtitle = "r = .32, n = 150")+
  theme_bw(base_size = 20) +
  theme(plot.subtitle = element_text(size = 12))


cor_n85b = ggplot(data.frame(x = seq(-.99, .99)), aes(x)) +
  stat_function(fun = function(x) r_sampling(x, r = -.85, 
                                             n = 150),
                geom = "area", fill = purples[3]) +
  scale_x_continuous(limits = c(-.99, .99)) +
  labs(subtitle = "r = -.85, n = 150")+
  theme_bw(base_size = 20) +
  theme(plot.subtitle = element_text(size = 12))


ggpubr::ggarrange(cor_n85, cor_32, cor_75, 
                  cor_n85b, cor_32b, cor_75b)
```


---
## Fisher's r to z' transformation

- Skewed sampling distribution will rear its head when:

    * $H_{0}: \rho \neq 0$
    * Calculating confidence intervals
    * Testing two correlations against one another

- r to z':

$$\large z^{'} = {\frac{1}{2}}ln{\frac{1+r}{1-r}}$$

???
ln = natural log
---
## Fisher’s r to z’ transformation

```{r, echo = F, fig.align='center', out.width="65%"}
r = seq(-.99,.99,.01)
z = psych::fisherz(r)
data.frame(r,z) %>%
  ggplot(aes(x = r, y = z)) +
  geom_line() +
  scale_x_continuous(expr(r))+
  scale_y_continuous(expr(z_r))+
  theme_bw(base_size = 20)
```


No longer bounded by 1 & -1

---
##  Computing confidence interval

1. Transform $r$ into $z'$
2. Compute CI as you normally would using $z'$
3. Revert back to $r$

$$ SE_z = \frac{1}{\sqrt{N-3}}$$

$$\large r = {\frac{e^{2z'}-1}{e^{2z'}+1}}$$

.small[*Note, `e` here stands for Euler's number. `exp(1)` is straight Euler's number or `e`, `exp(2)` is Euler's number squared or `e^2`*]
---

In a sample of 42 students, you calculate a correlation of 0.44 between hours spent outside on Saturday and self-rated health. What is the precision of your estimate?

```{r, echo = F}
r = .44
N = 42
z = fisherz(r)
se = 1/(sqrt(N-3))
ct = qt(p = .975, df = N-2, lower.tail = T)
lbz = z-(se*ct)
ubz = z+(se*ct)
lb = fisherz2r(lbz)
ub = fisherz2r(ubz)
```

$$z^{'} = {\frac{1}{2}}ln{\frac{1+.44}{1-.44}} = `r round(z,2)`$$
$$SE_z = \frac{1}{\sqrt{42-3}} = `r round(se,2)`$$

$$CI_{Z_{LB}} = `r round(z,2)`-(`r round(ct,3)`)`r round(se,2)` = `r round(lbz,2)`$$

$$CI_{Z_{UB}} = `r round(z,2)`+(`r round(ct,3)`)`r round(se,2)` = `r round(ubz,2)`$$



.tiny[Why is it 2.021 instead of 1.96? Because we're using a *t* distribution with N-2 degrees of freedom. `qt(p = .975, df = N-2, lower.tail = T)` = `r round(qt(p = .975, df = N-2, lower.tail = T), digits = 3)`]

---


$$CI_{r_{LB}} = {\frac{e^{2(`r round(lbz,2)`)}-1}{e^{2(`r round(lbz,2)`)}+1}} = `r round(lb,2)`$$

$$CI_{r_{UB}} = = {\frac{e^{2(`r round(ubz,2)`)}-1}{e^{2(`r round(ubz,2)`)}+1}} = `r round(ub,2)`$$

---
## How to do in R

```{r, eval=FALSE}
library(psych)
fisherz(r)
fisherz2r(z)
```

---

## Two independent group test

- Does the correlation in group 1 differ from the correlation in group 2?

$$ H_A: \rho_1 = \rho_2 $$

$$ H_A: \rho_1 \neq \rho_2 $$

- Normally distributed

$$Z = \frac{z_1^{'}- z_2^{'}}{se_{z_1-z_2}}$$

---
### Comparing two correlations

Again, we use Fisher’s $r$ to $z’$ transformation. Here, we're transforming the correlations into $z'$s, then using the difference between $z$'s to calculate the test statistic. 

$$Z = \frac{z_1^{'}- z_2^{'}}{se_{z_1-z_2}}$$

$$se_{z_1-z_2} = \sqrt{se_{z_1}+se_{z_2}} = \sqrt{\frac{1}{n_1-3}+\frac{1}{n_2-3}}$$

- But probably best to do this test in another framework (e.g., GLM via interaction or SEM)
---

### Example

Replication of Hill et al. (2012) where they found that the correlation between narcissism and happiness was greater for young adults compared to older adults

.pull-left[
### Young adults
$$N = 327$$
$$r = .402$$
]

.pull-right[
### Older adults
$$N = 273$$
$$r = .283$$
]



$$H_0:\rho_1 = \rho_2$$
$$H_1:\rho_1 \neq \rho_2$$

---

```{r, echo = F}
z1 = fisherz(.402)
z2 = fisherz(.283)
n1 = 327
n2 = 273
se = sqrt(1/(n1-3) + 1/(n2-3))

zstat = (z1-z2)/se
```


$$z_1^{'} = {\frac{1}{2}}ln{\frac{1+.402}{1-.402}} = `r round(z1,3)`$$

$$z_2^{'} = {\frac{1}{2}}ln{\frac{1+.283}{1-.283}} = `r round(z2,3)`$$

$$se_{z_1-z_2} = \sqrt{\frac{1}{327-3}+\frac{1}{273-3}} = `r round(se,3)`$$

$$\text{Test statistic} = \frac{z_1^{'}-z_2^{'}}{se_{z_1-z_2}} = \frac{`r round(z1,3)`- `r round(z2,3)`}{`r round(se,3)`} = `r round(zstat,3)`$$

```{r}
pnorm(abs(zstat), lower.tail = F)*2
```

.small[*Note: more examples at end of slides, after "next time"*]
---
## Effect size

- The strength of relationship between two variables

- $\eta^2$, Cohen’s d, Cohen’s f, hedges g, $R^2$ , Risk-ratio, etc

- Significance is a function of effect size and sample size

- Statistical significance $\neq$ practical significance

---
## Effect size
How big is practical?

- Cohen (.1, .3, .5)
- Meyer & Hemphill .3 is average

---
### What is the size of the correlation?
- Chemotherapy and breast cancer survival?
- Batting ability and hit success on a single at bat?
- Antihistamine use and reduced sneezing/runny nose?
- Combat exposure and PTSD?
- Ibuprofen on pain reduction?
- Gender and weight?
- Therapy and well being?
- Observer ratings of attractiveness?
- Gender and arm strength?


---
### What is the size of the correlation?
- Chemotherapy and breast cancer survival? (.03)
- Batting ability and hit success on a single at bat? (.06)
- Antihistamine use and reduced sneezing/runny nose? (.11)
- Combat exposure and PTSD? (.11)
- Ibuprofen on pain reduction? (.14)
- Gender and weight? (.26)
- Therapy and well being? (.32)
- Observer ratings of attractiveness? (.39)
- Gender and arm strength? (.55)


---
## Questions to ask yourself:
- What is your N?
- What is the typical effect size in the field?
- Study design?
- What is your DV?
- Importance?

---
## Correlation matrices

Correlations are both a descriptive and an inferential statistic. As a descriptive statistic, they're useful for understanding what's going on in a larger dataset. 

Like we use the `summary()` or `describe()` (psych) functions to examine our dataset _before we run any infernetial tests_, we should also look at the correlation matrix. 

---

```{r}
library(psych)
data(bfi)
head(bfi)
```

---

```{r}
cor(bfi)
```

---

```{r}
round(cor(bfi, use = "pairwise"),2)
```

---

```{r}
round(cor(bfi, use = "complete"),2)
```

---

With **pairwise deletion**, different sets of cases contribute to different correlations.  That maximizes the sample sizes, but can lead to problems if the data are missing for some systematic reason.

**Listwise deletion** ("complete cases") doesn't have the same issue of biasing correlations, but does result in smaller samples and potentially limited generalizability.

A good practice is comparing the different matrices; if the correlation values are very different, this suggests that the missingness that affects pairwise deletion is systematic.

---

```{r}
round(cor(bfi, use = "pairwise")- cor(bfi, use = "complete"),2)
```
---

## Visualizing correlations

For a single correlation, best practice is to visualize the relationship using a scatterplot. A best fit line is advised, as it can help clarify the strength and direction of the relationship. 

[http://guessthecorrelation.com/](http://guessthecorrelation.com/)


See also: [Interpreting Correlations](https://rpsychologist.com/correlation/)
---

```{r, echo = F, warning = FALSE, message=FALSE}
library(datasauRus)
datasaurus_dozen %>%
  filter(dataset == "away") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```


---

```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "h_lines") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---

```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "x_shape") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---

```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "circle") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```


---
```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "wide_lines") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```


---
```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "bullseye") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---
```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "star") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 2)+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---
```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "dino") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 2)+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---

## Visualizing correlation matrices

A single correlation can be informative; a correlation matrix is more than the sum of its parts. 

Correlation matrices can be used to infer larger patterns of relationships. You may be one of the gifted who can look at a matrix of numbers and see those patterns immediately. Or you can use **heat maps** to visualize correlation matrices. 

```{r, results = 'hide', message=FALSE, warning=FALSE}
library(corrplot)
```

---

```{r}
corrplot(cor(bfi, use = "pairwise"), method = "square")
```

---

![](images/comm plot-1.png)

.small[
[Beck, Condon, & Jackson, 2019](https://psyarxiv.com/857ev/)
]
---
## Other correlation tests:
1. Set of correlations
2. Dependent correlations (i.e., within same group). These are more easily tested via Structural Equation Modeling (SEM)
3. Intra Class Correlation (ICC)

- Again, best to do these tests in another framework (e.g., interaction, SEM, MLM)

---
### Factors that influence *r* 
1. Restriction of range (GRE scores and success)

2. Very skewed distributions (smoking and health)

3. Non-linear associations

4. Measurement overlap (modality and content)

5. Reliability

6. Outliers
---
## Reliability


Which would you rather have?

  - 1-item final exam versus 30-item?

  - assessment via trained clinician vs tarot cards?

  - fMRI during minor earthquake vs no earthquake?

--

All measurement includes error

- Score = true score + measurement error (CTT version)

- Reliability assesses the consistency of measurement; high reliability indicates less error

---

## Reliability

- Error is random; it cannot correlate with something

- Because we don't measure our variables perfectly, we get lower correlations compared to true correlations

- If we want to have a valid measure it better be a reliable measure

---
## Reliability

- Think of reliability as a correlation with a measure and itself in a different world, at a different time, or a different but equal version

$$\large r_{XX}$$

---
### Reliability

- True score variance divided by observed variance

$$r_{XX} = \frac{\sigma^2_{T}}{\sigma^2_{T} + \sigma^2_{E}}$$

- How do you assess true score variance?

$$\large r_{XY} = r_{X_{T} Y_{T}} {\sqrt{r_{XX}r_{YY}}}$$
- We can look at our observed correlation ( $r_{XY}$ ) and our reliability coefficients ( $r_{XX}$ and $r_{YY}$ ), and solve

$$\large .3 = r_{X_{T} Y_{T}} {\sqrt {(.70) (.70)}}$$

---
## Reliability

$$\large r_{X_{T} Y_{T}} =  {\frac {r_{XY}} {\sqrt{r_{XX}r_{YY}}}}$$


$$\large r_{X_{T} Y_{T}} = {\frac {.30} {\sqrt{(.70)(.70)}}} = .42$$
--

- CAVEAT: Reliabilities are also estimates. They can be wrong and there are lots of ways to get a reliability coefficient. The correlation you calculate ( $r_{X_{T} Y_{T}}$ ) is the highest correlation it could possibly be -- not the actual "theoretically true" correlation.

---
## Most common ways to assess

- Cronbach's $\alpha$
```{r, eval=FALSE}
library (psych)
alpha(measure)
## Gives average split half correlation
## Can tell you if you are assessing a single construct
```
- Test - retest reliability (r, ICC)
- Inter-rater reliability (kappa, ICC)

---
## Reliability

- If you are going to measure something, do it well

- Applies to ALL IVs and DVs, and all designs

- Remember this when interpreting other research

---
## Types of correlations

- Many ways to get at relationship between two variables

- Statistically the different types are almost exactly the same

- Exist for historical reasons

---

## Types of correlations
1. Point Biserial
    +  continuous and dichtomous
2. Phi coefficient
    + both dichotomous
3. Spearman rank order
    + ranked data (nonparametric)
4. Biserial (assumes dichotomous is continous)
5. Tetrachoric (assumes dichotomous is continous)
    + both dichotomous
6. Polychoric (assumes continous)
    + ordinal

---

class: inverse

## Next time....

Univariate regression

(remaining slides include more examples, if you want some practice)

---
### Example

The correlation between midterm exam grades and final exam grades was .56. The class size was 104. Is this statistically significant?

```{r, echo = F}
N = 104
r = .56

se = sqrt((1-r^2)/(N-2))

p1 = pt(r/se, df = N-2, lower.tail = F)
```


--
### Using t-method

$$\large SE_r = \sqrt{\frac{1-r^2}{N-2}} = \sqrt{\frac{1-.56^2}{104-2}} = `r round(se,2)`$$
$$\large t = \frac{r}{SE_r} = \frac{`r round(r,2)`}{`r round(se,2)`} = `r round(r/se,2)`$$

---


.pull-left[
Probability of getting a *t* statistic of `r round(r/se,2)` or greater is $3.19 x 10^{-10}$


Probability of getting *t* statistic of `r round(r/se,2)` or more extreme is $6.38 x 10^{-10}$
]

.pull-right[

```{r, echo = F, message = F, warning = F, results='hide'}
library(tidyverse)
data.frame(x = c(-3,7)) %>%
  ggplot(aes(x = x)) +
  stat_function(fun = function(x) dt(x = x, df = N-2), geom = "line") +
  geom_vline(aes(xintercept = r/se), color = "purple") + 
  ggtitle("t distribution (DF = 102)")+
  theme_bw()
```
]
---

### Example

The correlation between midterm exam grades and final exam grades was .56. The class size was 104. Is this statistically significantly different from .40?

--

```{r, echo = F}
r = .56
N = 104
null = .40
zr = psych::fisherz(r)
znull = psych::fisherz(null)
se = 1/sqrt(N-3)
stat = (zr-znull)/se
```

$$\large z^{'} = {\frac{1}{2}}ln{\frac{1+r}{1-r}}= {\frac{1}{2}}ln{\frac{1+`r r`}{1-`r r`}} = `r round(zr,2)`$$
$$\large z^{'}_{H_0} = {\frac{1}{2}}ln{\frac{1+r}{1-r}}= {\frac{1}{2}}ln{\frac{1+`r null`}{1-`r null`}} = `r round(znull,2)`$$
$$ SE_z = \frac{1}{\sqrt{`r N`-3}} = `r round(se,2)`$$

---

$$Z_{\text{statistic}} = \frac{z'-\mu}{SE_z}=\frac{`r round(zr,2)`-`r round(znull,2)`}{`r round(se,2)`} = `r round(stat,2)`$$

```{r}
stat
pnorm(stat, lower.tail = F)
pnorm(stat, lower.tail = F)*2
```
