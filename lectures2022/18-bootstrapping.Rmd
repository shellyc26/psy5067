---
title: 'Random & Resampling'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, rladies, rladies-fonts, "my-theme.css"]
    incremental: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r, echo = F, message = F, warning = F}
library(tidyverse)
```


## Last time...

Logistic Regression 

## Today

Robust estimation

* Weighted Least Squares
* __Resampling methods, Bootstrapping__

---

## Homoscedasticity

Homoscedasticity is the assumption that the variance of Y is constant across all levels of a predictor.

```{r, echo = F}
set.seed(031020)
N = 80
n.per = round(N/4)
y.x1 = rnorm(n = n.per, mean = 50, sd = 10)
y.x2 = rnorm(n = n.per, mean = 40, sd = 7)
y.x3 = rnorm(n = n.per, mean = 30, sd = 5)
y.x4 = rnorm(n = n.per, mean = 20, sd = 2)

y = c(y.x1, y.x2, y.x3, y.x4)

x.list = sapply(c(1:4), function(x) rnorm(n = n.per, mean = x, sd = .2))
xvar = as.vector(x.list)

Data = data.frame(calm = xvar, 
                  problems = round(y/10))
```

```{r, message = F, fig.height = 4}
ggplot(Data, aes(x = calm, y = problems)) +
  geom_jitter(alpha = .5)
```

---

## Weighted least squares

Weighted least squares (WLS) is a commonly used remedial procedure for heteroscedasticity (or failing to meet the assumption of homoscedasticity).

In an ordinary least squares (OLS) approach, each case in the dataset is given equal weight. WLS assigns each case a weight $w_i$, depending upon the precision of the observation of Y in that case. For observations for which the variance around the residuals around the regression line is low, the case is given a high weight.

---

Recall that an OLS estimation chooses values of $b_0$ and $b_1$ that minimizes the sum of squared residuals:

$$\large \text{min}(\sum e^2_i) = \text{min} \sum (Y_i-b_1X_i-b_0)^2$$

In a WLS approach, weights are taken into account, such that the values of $b_0$ and $b_1$ are chosen to minimize the sum of the **weighted** squared residuals:

$$\large \text{min}(\sum w_ie^2_i) = \text{min} \sum w_i(Y_i-b_1X_i-b_0)^2$$

The value of the weights is the inverse of the conditional variance of the residuals in the population corresponding to the specified value of X:

$$\large w_i = \frac{1}{\sigma^2_{Y-\hat{Y}|X}}$$
---

The value of $\sigma^2_{Y-\hat{Y}|X}$, the variance of the residuals in the population conditional on X, is not known and must be estimated. A common procedure for estimating weights is to estimate the usual OLS regression equation, square the residuals, and then regress the squared residuals onto X. The weight is then estimated as the inverse of the predicted value for a case. 

Using our own data:

```{r}
ols.model = lm(problems ~ calm, data = Data)
library(broom)
ols_aug = augment(ols.model)
head(ols_aug)
```
---

```{r}
# square residuals
ols_aug$resid_sq = ols_aug$.resid^2
# regress squared resid on predictor
weight.mod = lm(resid_sq ~ calm, data = ols_aug)
```

.pull-left[
```{r}
coef(ols.model)
```

]

.pull-right[
```{r}
coef(weight.mod)
```

]

```{r}
# extract predicted values
pred.resid = predict(weight.mod)
head(pred.resid)
# find inverse of predicted values
# use absolute value if some of your predicted values are negative  
est.weights = 1/abs(pred.resid) 
head(est.weights)
```


---

```{r}
wls.model = lm(problems ~ calm, data = Data, weights = est.weights)
```


#### OLS solution
```{r}
tidy(ols.model)
```

#### WLS solution
```{r}
tidy(wls.model)
```

---

```{r, echo = F, fig.width=10, fig.height=8, warning = FALSE, message = FALSE}
Data %>%
  ggplot(aes(x = calm, y = problems)) +
  geom_point(alpha = .5, position = position_jitter(width = 0, height = .1)) +
  geom_smooth(method = "lm", se = F, aes(color = "OLS")) +
  geom_smooth(method = "lm", se = F, aes(color = "WLS", weight = est.weights))
```


---
## When to use WLS

WLS is a **robust** method of estimation. "**Robust statistics** are statistics with good performance for data drawn from a wide range of probability distributions, especially for distirbutions that are not normal." .small[_(this quote taken straight from the [Wikipedia page](https://en.wikipedia.org/wiki/Robust_statistics))_]

Use when...
- Dealing with heteroscedasticity
- You know already that points should not be treated equally (some should be weighed more than others); do you know that some of your data was measured with less error than other data?


---
## What is Sampling?

- It is nearly impossible to survey every person in the population we are interested in


- We take a random sample from the population

- We calculate a statistic (mean, median, etc.)

- *In theory*, we would draw another random sample. But we often can't do that so we

- Luckily, a lot of our statistics follow well-defined distributions, and we can use those distribution properties to estimate the population parameters

---
## What is Resampling?

- The problem with sampling as we defined previously is that we only actually have 1 estimate of the population parameter

- We don't have a very good sense of the precision of that estimated population parameter

- *In theory*, we can use something like the standard error of the mean, but that also has a lot of assumptions built in. 

--

- **Resampling** methods let us re-estimate our population parameter multiple times *from the same dataset*

- This gives us more accurate estimates and more thorough picture of precision

---
## Resampling

- Bootstrapping
- Jacknifing (very similar, won't cover)
- Permutation testing (you will cover in Research Methods with Ian)

---
## Bootstrapping

What if you violate the normality assumption of regression?

--

A **bootstrapping** approach is useful when the theoretical sampling distribution for an estimate is unknown or unverifiable. In other words, if you have reason to suspect that either the variables in your model are not normally distributed, then bootstrapping can help ensure non-bias estimation and appropriately sized confidence intervals. 

--

It is another way to get **robust** statistics. Used most frequently for CIs, but also for other more basic stats.

---

## Bootstrapping

We either don't know the theoretical sampling distribution, or at the very least, we can't verify it.

--

We assume that the sample we have is representative of some population

--

**We build the sampling distribution empirically by randomly sampling with replacement from the sample**

---

### Illustration

Imagine you had a sample of 6 people: Rachel, Monica, Phoebe, Joey, Chandler, and Ross. To bootstrap their heights, you would draw from this group many samples of 6 people *with replacement*, each time calculating the average height of the sample.

```{r, echo = F}
friends = c("Rachel", "Monica", "Phoebe", "Joey", "Chandler", "Ross")
heights = c(65, 65, 68, 70, 72, 73)
names(heights) = friends
(sample1 = sample(friends, size = 6, replace = T)); mean(heights[sample1])
(sample1 = sample(friends, size = 6, replace = T)); mean(heights[sample1])
(sample1 = sample(friends, size = 6, replace = T)); mean(heights[sample1])
(sample1 = sample(friends, size = 6, replace = T)); mean(heights[sample1])
(sample1 = sample(friends, size = 6, replace = T)); mean(heights[sample1])
(sample1 = sample(friends, size = 6, replace = T)); mean(heights[sample1])
(sample1 = sample(friends, size = 6, replace = T)); mean(heights[sample1])
(sample1 = sample(friends, size = 6, replace = T)); mean(heights[sample1])
```
???


```{r}
heights
```

---
### Illustration

```{r}
boot = 10000
friends = c("Rachel", "Monica", "Phoebe", "Joey", "Chandler", "Ross")
heights = c(65, 65, 68, 70, 72, 73)
sample_means = numeric(length = boot)
for(i in 1:boot){
  this_sample = sample(heights, size = length(heights), replace = T)
  sample_means[i] = mean(this_sample)
}
```

---

## Illustration 
```{r, echo = F, message = F, fig.width = 10, fig.height=6, warning = F}
library(ggpubr)
mu = mean(heights)
sem = sd(heights)/sqrt(length(heights))
cv_t = qt(p = .975, df = length(heights)-1)

bootstrapped = data.frame(means = sample_means) %>%
  ggplot(aes(x = means)) + 
  geom_histogram(color = "white") +
  geom_density() +
  geom_vline(aes(xintercept = mean(sample_means), color = "mean"), size = 2) +
  geom_vline(aes(xintercept = median(sample_means), color = "median"), size = 2) +
  geom_vline(aes(xintercept = quantile(sample_means, probs = .025), color = "Lower 2.5%"), size = 2) +
    geom_vline(aes(xintercept = quantile(sample_means, probs = .975), color = "Upper 2.5%"), size = 2) +
  scale_x_continuous(limits = c(mu-3*sem, mu+3*sem))+
  ggtitle("Bootstrapped distribution") +
  cowplot::theme_cowplot()


from_prob = data.frame(means = seq(from = min(sample_means), to = max(sample_means))) %>%
  ggplot(aes(x = means)) +
  stat_function(fun = function(x) dnorm(x, m = mu, sd = sem)) + 
  geom_vline(aes(xintercept = mean(heights), color = "mean"), size = 2) +
  geom_vline(aes(xintercept = median(heights), color = "median"), size = 2) +
  geom_vline(aes(xintercept = mu-cv_t*sem, color = "Lower 2.5%"), size = 2) +
  geom_vline(aes(xintercept = mu+cv_t*sem, color = "Upper 2.5%"), size = 2) +scale_x_continuous(limits = c(mu-3*sem, mu+3*sem))+  
  ggtitle("Distribution from probability theory") +
  cowplot::theme_cowplot()

ggarrange(bootstrapped, from_prob, ncol = 1)
```

---
### Example

A sample of 216 response times. What is their central tendency and variability?

There are several candidates for central tendency (e.g., mean, median) and for variability (e.g., standard deviation, interquartile range).  Some of these do not have well understood theoretical sampling distributions.

For the mean and standard deviation, we have theoretical sampling distributions to help us, provided we think the mean and standard deviation are the best indices. For the others, we can use bootstrapping.

---
```{r, echo = F, message=F, fig.width=10, fig.height=8}
library(tidyverse)
set.seed(03102020)
response = rf(n = 216, 3, 50) 
response = response*500 + rnorm(n = 216, mean = 200, sd = 100)

values = quantile(response, 
                  probs = c(.025, .5, .975))
mean_res = mean(response)

data.frame(x = response) %>%
  ggplot(aes(x)) +
  geom_histogram(aes(y = ..density..), binwidth = 150, fill = "lightgrey", color = 
                   "black")+
  geom_density()+
  geom_vline(aes(xintercept = values[1], 
                 color = "Lower 2.5%"), size = 2)+
  geom_vline(aes(xintercept = values[2], 
                 color = "Median"), size = 2)+
  geom_vline(aes(xintercept = values[3], 
                 color = "Upper 2.5%"), size = 2)+
  geom_vline(aes(xintercept = mean_res, 
                 color = "Mean"), size = 2)+
  labs(x = "Reponse time (ms)", title = "Response Time Distribution") + cowplot::theme_cowplot(font_size = 20)
```

---
### Bootstrapping

Before now, if we wanted to estimate the mean and the 95% confidence interval around the mean, we would find the theoretical sampling distribution by scaling a t-distribution to be centered on the mean of our sample and have a standard deviation equal to $\frac{s}{\sqrt{N}}.$ But we have to make many assumptions to use this sampling distribution, and we may have good reason not to.  

Instead, we can build a population sampling distribution empirically by randomly sampling with replacement from the sample.


---

### Response time example
```{r}
boot = 10000
response_means = numeric(length = boot)
for(i in 1:boot){
  sample_response = sample(response, size = 216, replace = T)
  response_means[i] = mean(sample_response)
}
```

```{r, echo = F, message = F, fig.width = 10, fig.height=6}
data.frame(means = response_means) %>%
  ggplot(aes(x = means)) + 
  geom_histogram(color = "white") +
  geom_density() +
  geom_vline(aes(xintercept = mean(response_means), color = "mean"), size = 2) +
  geom_vline(aes(xintercept = median(response_means), color = "median"), size = 2) +
  geom_vline(aes(xintercept = quantile(response_means, probs = .025), color = "Lower 2.5%"), size = 2) +
    geom_vline(aes(xintercept = quantile(response_means, probs = .975), color = "Upper 2.5%"), size = 2) +
  cowplot::theme_cowplot()
```


---

```{r}
mean(response_means)
median(response_means)

quantile(response_means, probs = c(.025, .975))
```

What about something like the median?

---
### bootstrapped distribution of the median
```{r}
boot = 10000
response_med = numeric(length = boot)
for(i in 1:boot){
  sample_response = sample(response, size = 216, replace = T)
  response_med[i] = median(sample_response)
}
```
.pull-left[
```{r echo=F,  message=FALSE}
data.frame(medians = response_med) %>%
  ggplot(aes(x = medians)) + 
  geom_histogram(aes(y = ..density..),
                 color = "white", fill = "grey") +
  geom_density() +
  geom_vline(aes(xintercept = mean(response_med), color = "mean"), size = 2) +
  geom_vline(aes(xintercept = median(response_med), color = "median"), size = 2) +
  geom_vline(aes(xintercept = quantile(response_med, probs = .025), color = "Lower 2.5%"), size = 2) +
    geom_vline(aes(xintercept = quantile(response_med, probs = .975), color = "Upper 2.5%"), size = 2) +
  cowplot::theme_cowplot()
```
]
.pull-right[
```{r}
mean(response_med)
median(response_med)
quantile(response_med, 
         probs = c(.025, .975))
```
]
---
### bootstrapped distribution of the standard deviation
```{r}
boot = 10000
response_sd = numeric(length = boot)
for(i in 1:boot){
  sample_response = sample(response, size = 216, replace = T)
  response_sd[i] = sd(sample_response)
}
```
.pull-left[
```{r echo=F,  message=FALSE}
data.frame(sds = response_sd) %>%
  ggplot(aes(x = sds)) + 
  geom_histogram(aes(y = ..density..),color = "white", fill = "grey") +
  geom_density() +
  geom_vline(aes(xintercept = mean(response_sd), color = "mean"), size = 2) +
  geom_vline(aes(xintercept = median(response_sd), color = "median"), size = 2) +
  geom_vline(aes(xintercept = quantile(response_sd, probs = .025), color = "Lower 2.5%"), size = 2) +
    geom_vline(aes(xintercept = quantile(response_sd, probs = .975), color = "Upper 2.5%"), size = 2) +
  cowplot::theme_cowplot()
```
]
.pull-right[
```{r}
mean(response_sd)
median(response_sd)
quantile(response_sd, 
         probs = c(.025, .975))
```
]

---

You can bootstrap estimates and 95% confidence intervals for *any* statistics you'll need to estimate. 

The `boot` package and function provides some help to speed this process along. Things you should learn how to do in R:

- learn to read a `for loop`
- learn to write your own function

```{r}
library(boot)

# function to obtain R-Squared from the data
rsq <- function(data, indices) {
  d <- data[indices,] # allows boot to select sample
  
  fit <- lm(mpg~wt+disp, data=d) # this is the code you would have run
  
  return(summary(fit)$r.square)
}
# bootstrapping with 10000 replications
results <- boot(data=mtcars, statistic=rsq,
   R=10000)
```

---
.pull-left[
```{r}
data.frame(rsq = results$t) %>%
  ggplot(aes(x = rsq)) +
  geom_histogram(color = "white", bins = 30) 
```
]

.pull-right[
```{r}
median(results$t)
boot.ci(results, type = "perc")
```
]

---

### Example 2

Samples of service waiting times for Verizon’s (ILEC) versus other carriers (CLEC) customers. In this district, Verizon must provide line service to all customers or else face a fine. The question is whether the non-Verizon customers are getting ignored or facing greater variability in waiting times.

```{r, message = F, warning = F, echo = 2}
library(here)
Verizon = read.csv(here("data/Verizon.csv"))
```

```{r, echo = F, fig.width = 10, fig.height = 4}
Verizon %>%
  ggplot(aes(x = Time, fill = Group)) + 
  geom_histogram(bins = 30) + 
  guides(fill = "none") +
  facet_wrap(~Group, scales = "free_y")
```

---

```{r, echo = F, fig.width = 10, fig.height = 6}
Verizon %>%
  ggplot(aes(x = Time, fill = Group)) + 
  geom_histogram(bins = 50, position = "dodge") + 
  guides(fill = "none")
table(Verizon$Group)
```

---

There's no world in which these data meet the typical assumptions of an independent samples t-test. To estimate mean differences we can use bootstrapping. Here, we'll resample with replacement separately from the two samples. 

```{r}
boot = 10000
difference = numeric(length = boot)

subsample_CLEC = Verizon %>% filter(Group == "CLEC")
subsample_ILEC = Verizon %>% filter(Group == "ILEC")

for(i in 1:boot){
  sample_CLEC = sample(subsample_CLEC$Time, 
                       size = nrow(subsample_CLEC), 
                       replace = T)
  sample_ILEC = sample(subsample_ILEC$Time, 
                       size = nrow(subsample_ILEC), 
                       replace = T)
  
  difference[i] = mean(sample_CLEC) - mean(sample_ILEC)
}
```

---

```{r echo=F,  message=FALSE, fig.width=10, fig.height=5}
data.frame(differences = difference) %>%
  ggplot(aes(x = differences)) + 
  geom_histogram(aes(y = ..density..),color = "white", fill = "grey") +
  geom_density() +
  geom_vline(aes(xintercept = mean(differences), color = "mean"), size = 2) +
  geom_vline(aes(xintercept = median(differences), color = "median"), size = 2) +
  geom_vline(aes(xintercept = quantile(differences, probs = .025), color = "Lower 2.5%"), size = 2) +
    geom_vline(aes(xintercept = quantile(differences, probs = .975), color = "Upper 2.5%"), size = 2) +
  cowplot::theme_cowplot()
```

The difference in means is `r round(median(difference),2)` $[`r round(quantile(difference, probs = .025),2)`,`r round(quantile(difference, probs = .975),2)`]$.

---

### Bootstrapping Summary

Bootstrapping can be a useful tool to estimate parameters when 
1. You've violated assumptions of the test (i.e., normality, homoscedasticity)
2. You have good reason to believe the sampling distribution is not normal, but don't know what it is
3. There are other oddities in your data, like very unbalanced samples 

This allows you to create a confidence interval around any statistic you want -- Cronbach's alpha, ICC, Mahalanobis Distance, $R^2$, AUC, etc. 
* You can test whether these statistics are significantly different from any other value -- how?

---

### Bootstrapping Summary

Bootstrapping will NOT help you deal with:

* Dependence between observations -- for this, you'll need to explicitly model dependence (more on this our last 2 lectures)

* Improperly specified models or forms -- use theory to guide you here

* Measurement error -- why bother?
---
class: inverse

## 2 more to go...

Yarkoni and Westfall (2017) and the lessons of machine learning

READ THE PAPER


2 more lectures + 1 review + 1 exam
